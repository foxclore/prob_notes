% !TEX root = ../main.tex
\section{Числові характеристики випадкових величин}
\subsection{Математичне сподівання}

\begin{definition}
    \emph{Математичним сподіванням} випадкової величини називається
    інтеграл Стілтьєса
    \begin{equation}\label{eq:e_xi}
        E\xi = \int_{-\infty}^{+\infty} x dF_\xi(x) = \begin{cases}
            \sum\limits_{k=1}^{n(\infty)} x_k P\left\{\xi = x_k\right\}, & \xi \text{ --- ДВВ} \\
            \int\limits_{-\infty}^{+\infty} x f_\xi(x)dx, & \xi \text{ --- НВВ}
        \end{cases}
    \end{equation}
\end{definition}
Інтеграл \eqref{eq:e_xi} має збігатися \emph{абсолютно}, інакше кажуть, що
випадкова величина не має математичного сподівання.
\begin{example}
    НВВ, розподілена за законом Коші з щільністю $f_\xi(x) = \frac{1}{\pi (1+x^2)}$
    не має математичного сподівання, бо інтеграл $\frac{1}{\pi}\int_{-\infty}^{+\infty} \frac{x}{1+x^2}dx$ 
    розбіжний.
\end{example}
Математичне сподівання --- ймовірнісне середнє значення випадкової величини.
Фізична інтерпретація --- центр мас системи точок $\left\{x_1, x_2, ..., x_n,...\right\}$ з масами $\left\{p_1, p_2, ..., p_n, ...\right\}$ (ДВВ) або стрижня,
розподіл маси в якому задано функцією щільності (НВВ).

\noindent \textbf{Властивості математичного сподівання:}
\begin{enumerate}
    \item Математичне сподівання константи --- сама константа, оскільки
    її можна інтерпретувати як ДВВ, що приймає єдине значення з ймовірністю $1$.
    
    $c = const, E c = c$.
    \item $E \left(c\cdot\xi\right) = c\cdot E\xi$ --- властивість рядів та інтегралів.
    \item $E\left( \xi_1 + \xi_2\right) = E\xi_1 + E\xi_2$.
    \begin{proof}[Доведення для ДВВ]
        $P\left\{\xi_1 = x_k\right\} = p_k$, $P\left\{\xi_2 = y_j\right\} = p_j$, $P\left\{\xi_1 + \xi_2 = x_k + y_j\right\} = p_{kj}$.
        $E\left( \xi_1 + \xi_2\right) = \sum_k \sum_j (x_k+y_j) p_k p_j =
        \sum_k x_k \sum_j p_{kj} + \sum_j y_j \sum_k p_{kj} = \sum_k x_k p_k + \sum_j y_j p_j = E\xi_1 + E\xi_2$.
    \end{proof}
\suspend{enumerate}
\begin{definition}
    Дві випадкові величини $\xi_1$ та $\xi_2$, задані на одному ймовірнісному просторі, називаються \emph{незалежними}, якщо
    $\forall x, y$ події $A=\left\{\omega : \; \xi_1(\omega) < x\right\}$ та
    $B=\left\{\omega : \; \xi_2(\omega) < y\right\}$ є незалежними.
    В цьому випадку $P\left\{\xi_1 < x, \xi_2 < y\right\} = P(A \cap B) = P(A)\cdot P(B) = F_{\xi_1}(x)\cdot F_{\xi_2}(y)$.
\end{definition}
\resume{enumerate}
    \item Якщо $\xi_1$ та $\xi_2$ незалежні, то $E\xi_1\xi_2 = E\xi_1 \cdot E\xi_2$.
    \begin{proof}[Доведення для ДВВ]
        Позначимо $p_{kj} = P\left\{\xi_1 = x_k, \xi_2 = y_j\right\}$.
        $E\xi_1\xi_2 = \sum_k \sum_j x_k y_j p_{kj} = \sum_k \sum_j x_k y_j p_k p_j = ( \sum_k x_k p_k)\cdot ( \sum_j y_j p_j) = E\xi_1 \cdot E\xi_2$.
    \end{proof}
    Властивості 3 та 4 для НВВ буде доведено в темі <<Функції випадкових аргументів>>.
\end{enumerate}

\begin{example}
    Обчислити математичне сподівання двох ДВВ:
    
    \begin{tabular}{c c}
        \begin{tabular}{c|c|c}
            $\xi_1$ & $-1$ & $1$ \\ 
            \hline
            $p$ & $1/2$ & $1/2$
        \end{tabular} &
        \begin{tabular}{c|c|c}
            $\xi_2$ & $-100$ & $100$ \\ 
            \hline
            $p$ & $1/2$ & $1/2$
        \end{tabular}
    \end{tabular}
    $E\xi_1 = -1\cdot \frac{1}{2} + 1\cdot \frac{1}{2} = 0, E\xi_2 = -100\cdot \frac{1}{2} + 100\cdot \frac{1}{2} = 0$.
    
    Цей приклад показує, що попри однакове значення математичного сподівання, можливі значення цих ДВВ знаходяться на різній відстані від нього.
\end{example}

\subsection{Дисперсія}
\begin{definition}
    \emph{Дисперсією} випадкової величини називається 
    \begin{equation}\label{eq:d_xi}
        D\xi = E\left(\xi-E\xi\right)^2 = \int_{-\infty}^{+\infty} \left(x-E\xi\right)^2 dF_\xi(x) = \begin{cases}
            \sum\limits_{k=1}^{n(\infty)} \left(x_k-E\xi\right)^2 P\left\{\xi = x_k\right\}, & \xi \text{ --- ДВВ} \\
            \int\limits_{-\infty}^{+\infty} \left(x-E\xi\right)^2 f_\xi(x)dx, & \xi \text{ --- НВВ}
        \end{cases}
    \end{equation}
\end{definition}
Дисперсія --- характеристика розсіювання випадкової величини навколо свого математичного сподівання.
Фізична інтерпретація --- момент інерції маси системи точок або стрижня (як і у випадку інтерпретації математичного сподівання)
відносно свого центру мас.

\begin{example}
    Обчислити дисперсії двох ДВВ:
    \begin{tabular}{c c}
        \begin{tabular}{c|c|c}
            $\xi_1$ & $-1$ & $1$ \\ 
            \hline
            $p$ & $1/2$ & $1/2$
        \end{tabular} &
        \begin{tabular}{c|c|c}
            $\xi_2$ & $-100$ & $100$ \\ 
            \hline
            $p$ & $1/2$ & $1/2$
        \end{tabular}
    \end{tabular}
    
    $D\xi_1 = (-1)^2\cdot \frac{1}{2} + 1^2\cdot \frac{1}{2} = 1, E\xi_2 = (-100)^2\cdot \frac{1}{2} + 100^2\cdot \frac{1}{2} = 10000$.
\end{example}

\noindent \textbf{Властивості дисперсії:}
\begin{enumerate}
    \item $D\xi \geq 0$, $\sqrt{D\xi} = \sigma_\xi$ --- \emph{середньоквадратичне (стандартне) відхилення}.
    \item $D\xi = 0 \Leftrightarrow \xi = const$, бо $D c = E(c - Ec)^2 = 0$.
    \item Для обчислення дисперсії більш зручною є наступна формула:

    $D\xi = E(\xi^2 - 2\xi E\xi +(E\xi)^2) = E\xi^2 - 2(E\xi)^2 + (E\xi)^2 = E\xi^2 - (E\xi)^2$. 
    \item $D(c\cdot \xi) = c^2\cdot D\xi$, оскільки $D(c\cdot \xi) = E\left(c\cdot\xi-E(c\cdot\xi\right))^2 = E\left(c\cdot\xi - c\cdot E\xi\right)^2 = c^2 \cdot E\left(\xi-E\xi\right)^2 = c^2 \cdot D\xi$.
\suspend{enumerate}
\begin{definition}
    \emph{Центрованою} випадковою величиною, що відповідає випадковій величині $\xi$, називається 
    випадкова величина $\mathring{\xi} = \xi - E\xi$. Для неї $E\mathring{\xi} = 0$ та 
    $E\mathring{\xi^2} = D\xi$.
\end{definition}
\resume{enumerate}
    \item Якщо випадкові величини $\xi_1$ та $\xi_2$ --- незалежні, то
    $D\left(\xi_1 + \xi_2\right) = D\xi_1 + D\xi_2$.
    \begin{proof}
        $D\left(\xi_1 + \xi_2\right) = E((\xi_1 + \xi_2) - E(\xi_1 + \xi_2))^2 = E(\mathring{\xi}_1 + \mathring{\xi}_2)^2 =
        E\mathring{\xi}_1^2 + 2E\mathring{\xi}_1\mathring{\xi}_2 + E\mathring{\xi}_2^2 = 
        E\mathring{\xi}_1^2 + 2E\mathring{\xi}_1 E\mathring{\xi}_2 + E\mathring{\xi}_2^2 =
        E\mathring{\xi}_1^2 + E\mathring{\xi}_2^2 = D\xi_1 + D\xi_2$.
    \end{proof}
\end{enumerate}

\subsection{Моменти випадкової величини}
\begin{definition}
    \emph{Початковим моментом k-того порядку} ($k \in \mathbb{N}$) ВВ $\xi$ називається
    \begin{equation}\label{eq:e_alpha_k}
        \alpha_k = E\xi^k = \int_{-\infty}^{+\infty} x^k dF_\xi(x) = \begin{cases}
            \sum\limits_{m=1}^{n(\infty)} x_m^k P\left\{\xi = x_m\right\}, & \xi \text{ --- ДВВ} \\
            \int\limits_{-\infty}^{+\infty} x^k f_\xi(x)dx, & \xi \text{ --- НВВ}
        \end{cases}
    \end{equation}
\end{definition}
\begin{definition}
    \emph{Центральним моментом k-того порядку} ($k \in \mathbb{N}$) ВВ $\xi$ називається
    \begin{equation}\label{eq:d_beta_k}
        \beta_k = E\mathring{\xi}^k = E\left(\xi-E\xi\right)^k = \int_{-\infty}^{+\infty} \left(x-E\xi\right)^k dF_\xi(x) = \begin{cases}
            \sum\limits_{m=1}^{n(\infty)} \left(x_m-E\xi\right)^k P\left\{\xi = x_m\right\}, & \xi \text{ --- ДВВ} \\
            \int\limits_{-\infty}^{+\infty} \left(x-E\xi\right)^k f_\xi(x)dx, & \xi \text{ --- НВВ}
        \end{cases}
    \end{equation}
\end{definition}

\noindent \textbf{Зв'язок між центральними та початковими моментами}

$\beta_k = E\left(\xi-E\xi\right)^k = E\left( \sum\limits_{j=0}^k C_k^j \xi^j (-1)^{k-j} (E\xi)^{k-j}\right) =
\sum\limits_{j=0}^k (-1)^{k-j} C_k^j E\xi^j (E\xi)^{k-j}$.

\noindent Отже, $\beta_k = \sum\limits_{j=0}^k (-1)^{k-j} C_k^j \alpha_j (\alpha_1)^{k-j}$.
Частинним випадком цієї формули є формула для дисперсії: $D\xi = \beta_2 = \alpha_2 - \alpha_1^2 = E\xi^2 - (E\xi)^2$.
\begin{exercise}
    Виразити через початкові моменти $\beta_3$ та $\beta_4$.
\end{exercise}
Розглядаються також абсолютні початкові моменти $E\vert \xi \vert ^k$, абсолютні центральні моменти $E\vert \xi - E\xi \vert ^k$
та факторіальні моменти $\gamma_k = E\left( \xi (\xi - 1) ... (\xi - k + 1)\right)$.


\noindent \textbf{Мода та медіана випадкової величини}
\begin{definition}
    \emph{Модою} ВВ ${Mo}_\xi$ називається абсциса точки максимуму щільності 
    розподілу ВВ у випадку неперервної ВВ та значення випадкової величини, ймовірність 
    виникнення якого є найбільшою --- у дискретному випадку.

    \emph{Унімодальний закон} --- такий, що має лише одну моду. 
    \emph{Полімодальний закон} --- такий, що має декілька мод.
\end{definition}
\begin{definition}
    Точка $x_0$ називається \emph{медіаною} $Me_\xi$, якщо 
    $P\left\{\xi < x_0\right\} = P\left\{\xi \geq x_0\right\} 
    = \frac{1}{2} \Leftrightarrow $
\end{definition}
\begin{remark}
    Медіана $Me_\xi$ є окремим випадком квантілей.
\end{remark}
\begin{definition}
    Точка $x_0$ називається \emph{квантилем q-го порядку} якщо $F_\xi(x_0) = q$.
\end{definition}

\noindent \textbf{Асиметрія та ексцес випадкової величини}
\begin{definition}
    \emph{Асиметрією} випадкової величини $As_\xi$ називається безрозмірна 
    числова характеристика, що дорівнює $As_\xi = \frac{\beta_3}{\sigma_\xi^3} = 
    \frac{E(\xi - E_\xi)^3}{(D_\xi)^{3/2}}$.

    Показує порушення чи наявність симетрії відносно її матсподівання.
\end{definition}
% В рот ебал вставлять картинку. ;)
\begin{definition}
    \emph{Ексцесом} випадкової величини $Ex_\xi$ називається безрозмірна 
    числова характеристика, що дорівнює $Ex_\xi = \frac{\beta_4}{\sigma_\xi^4} = 
    \frac{E(\xi - E_\xi)^4}{(D_\xi)^{2}}$.

    Ця числова характеристика показує, наскільки швидко ми прямуємо до точки 
    максимуму
\end{definition}
% В рот ебал вставлять картинку. ;)