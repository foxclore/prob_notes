% !TEX root = ../main.tex
\section{Умовна ймовірність та її застосування}

\subsection{Поняття умовної ймовірності}
Припустимо, що спостерігається деякий експеримент, що описується класичною моделлю, а для події $B$ $\P(B)>0$.
\emph{Умовна ймовірність} $\P(A/B)$ --- це ймовірність події $A$ за умови, що відбулась подія $B$.
Наприклад, експеримент --- витягання карт з колоди, $A=\left\{\text{витягнуто даму пік}\right\}$, $B=\left\{\text{витягнуто карту чорної масті}\right\}$.

Оскільки експеримент описується класичною моделлю, то можемо позначити $m_B$ та $m_{A\cap B}$ кількості елементарних подій, що сприяють появами подій $B$ та $A \cap B$ відповідно.
Тоді $\P(A/B) = \frac{m_{A\cap B}}{m_B} = \frac{m_{A\cap B}/n}{m_B/n} = \frac{\P(A\cap B)}{\P(B)}$, де $n = \mathrm{card}(\Omega)$.

Для довільних ймовірнісних просторів ця формула вводиться як означення умовної ймовірності.
\begin{definition}
    Якщо подія $B$ має додатну ймовірність $\P(B)>0$, то \emph{умовна ймовірність} події $A$ за умови, що відбулась подія $B$,
    обчислюється за формулою 
    \begin{equation}\label{eq:cond_prob}
        \P(A/B) = \frac{\P(A\cap B)}{\P(B)}
    \end{equation}
\end{definition}
\noindent \textbf{Властивості умовної ймовірності:}
\begin{enumerate}
    \item $\P(A/B) \geq 0$.
    \item $\P(\Omega /B) = \P(B/B) = 1$.
    \item $ \forall A_1, A_2, ... , A_n, ... \in \mathcal{F}: A_i \cap A_j = \varnothing \text{ при } i \neq j : \\
    \P\left(\left(\bigcup\limits_{n=1}^{\infty} A_n\right)/B\right) = \frac{\P\left(\left(\bigcup\limits_{n=1}^{\infty} A_n\right)\cap B\right)}{\P(B)} = \frac{\P\left(\bigcup\limits_{n=1}^{\infty} (A_n\cap B)\right)}{\P(B)} \overset{\text{\textbf{P3}}}{=} \frac{\sum\limits_{n=1}^{\infty} \P(A_n \cap B)}{\P(B)} = \sum\limits_{n=1}^{\infty} \P(A_n/B)$.
\end{enumerate}
\vspace{1em}
Таким чином, для умовної ймовірності виконуються аксіоми \textbf{P1}, \textbf{P2} та \textbf{P3}. 
\begin{exercise}
    Нехай $\left\{ \Omega, \mathcal{F}, \P\right\}$ --- деякий ймовірнісний простір, а $B\in\mathcal{F}$ --- деяка
    подія з $\P(B) > 0$. Перевірити, що для простору елементарних подій $\Omega_B$, що складається
    з усіх елементарних подій, які містить $B$, $\mathcal{F}_B = \left\{ A\cap B : A \in \mathcal{F} \right\}$ є $\sigma$-алгеброю.
    Разом з вже доведеними властивостями умовної ймовірності $\P(\cdot / B)$ це означає, що $\left\{ \Omega_B, \mathcal{F}_B, \P(\cdot / B)\right\}$
    також є ймовірнісним простором, в якому $B$ є вірогідною подією.
\end{exercise}

\subsection{Незалежність подій}
\begin{definition}
    Дві події $A$ та $B$ називаються \emph{незалежними}, якщо
    \begin{equation}\label{eq:indep_events}
        \P(A/B) = \P(A) \text{ або } \P(B/A) = \P(B)
    \end{equation}
    Це означає, що на появу однієї події не впливає поява іншої.
\end{definition}
\noindent \textbf{Властивості незалежних подій:}
\begin{enumerate}
    \item Якщо події $A$ та $B$ незалежні, то $\P(A\cap B) = \P(A)\cdot \P(B)$. Це випливає з \eqref{eq:cond_prob} та \eqref{eq:indep_events}.
    \begin{remark}
        Несумісні події залежні, якщо їх ймовірності не нульові.
    \end{remark}

    \item Якщо події $A$ та $B$ незалежні, то пари $A$ та $\overline{B}$, 
    $\overline{A}$ та $B$, $\overline{A}$ та $\overline{B}$ --- теж незалежні.
    \begin{proof}
        Доведемо спочатку для пари $A$ та $\overline{B}$. $\P(A\cap \overline{B}) = \P(A\setminus (A\cap B)) = \P(A) - \P(A\cap B) = \P(A) - \P(A)\cdot \P(B) = \P(A)\cdot (1 - \P(B)) = \P(A)\cdot \P(\overline{B})$.
        Отже, $A$ та $\overline{B}$ --- незалежні. Доведення для інших пар аналогічне.
    \end{proof}
    \item Якщо $A$ та $B$, $A$ та $C$ незалежні, а $B$ та $C$ несумісні, то $A$ та $B\cup C$ --- незалежні події.
    \begin{proof}
        $\P(A\cap (B \cup C)) = \P((A \cap B) \cup (A \cap C)) = \P(A\cap B) + \P(A\cap C) = \P(A)\cdot \P(B) + \P(A)\cdot \P(C) = \P(A) \cdot (\P(B) + \P(C)) = \P(A)\cdot \P(B\cup C).$
    \end{proof}
\end{enumerate}

\begin{definition}
    Події $A_1, A_2, ..., A_n \in \mathcal{F}$ називаються \emph{незалежними у сукупності}, якщо
    \begin{equation}\label{eq:indep}
        \forall i_1, i_2, ..., i_k \in \{1,...,n\}: \P\left(\bigcap\limits_{j=1}^k A_{i_j}\right) = \prod\limits_{j=1}^k \P\left(A_{i_j}\right)
    \end{equation}
    Зокрема:
    \nopagebreak
    \begin{enumerate}
        %\nopagebreak
        \item $\forall i, j \in {1,..,n}, i\neq j: \P(A_i \cap A_j) = \P(A_i)\cdot \P(A_j)$ --- попарна незалежність.
        \item $\P\left(\bigcap\limits_{k=1}^n A_k\right) = \prod\limits_{k=1}^n \P(A_k)$.
    \end{enumerate}
\end{definition}
\begin{remark}
    Із незалежності подій у сукупності випливає попарна незалежність, 
    але наслідку в інший бік, взагалі кажучи, немає.
\end{remark}
\begin{example}[приклад Бернштейна]
    Дитина кидає на підлогу тетраедр, три грані якого розфарбовані відповідно зеленим, синім та червоним кольором,
    а четверта --- усіма трьома кольорами. Введемо події $\text{З}, \text{С}, \text{Ч}$, які означають, що випала грань,
    на якій є зелений, синій або червоний колір відповідно.

    $\P(\text{З}) = \P(\text{С}) = \P(\text{Ч}) = \frac{1}{2}$, 
    $\P(\text{З} \cap \text{С}) = \P(\text{З} \cap \text{Ч}) = \P(\text{С} \cap \text{Ч}) = \frac{1}{4}$,
    тому ці події є попарно незалежними. Але $\P(\text{З} \cap \text{С} \cap \text{Ч}) = \frac{1}{4} \neq \frac{1}{8}$,
    тому незалежності у сукупності немає.
\end{example}

\subsection{Теореми додавання та множення}
\noindent\textbf{Теорема множення.} 
З формули \eqref{eq:cond_prob} випливає, що
\begin{equation}\label{eq:mult_for_2}
    \P(A\cap B) = \P(A)\cdot \P(B/A) = \P(B) \cdot \P(A/B)
\end{equation}
Для трьох подій: $\P(A \cap B \cap C) = \P(A\cap B) \cdot \P(C/(A\cap B)) = \P(A)\cdot \P(B/A) \cdot \P(C/(A\cap B))$.
За методом математичної індукції неважко довести, що
\begin{equation}\label{eq:mult_for_n}
    \P\left( \bigcap\limits_{k=1}^{n} A_k\right) = \P\left(A_1\right) \cdot \P\left(A_2/A_1\right) \cdot \P\left( A_3 / \left( A_1 \cap A_2\right)\right) \cdot ... \cdot \P\left( A_n / \left( \bigcap\limits_{k=1}^{n-1} A_k\right) \right)
\end{equation}

Якщо $A_1, A_2, ..., A_n \in \mathcal{F}$ --- незалежні у сукупності, то, як було зазначено, з \eqref{eq:indep} випливає, що
формула \eqref{eq:mult_for_n} спрощується до $\P\left(\bigcap\limits_{k=1}^n A_k\right) = \prod\limits_{k=1}^n \P(A_k) = \prod\limits_{k=1}^n \left(1-\P(\overline{A_k})\right)$.

\begin{example}
    На 10 картках написано по одній букві так, що можна скласти слово <<математика>>.
    Дитина навмання витягує без повернення по одній картці. Яка ймовірність того, що після витягання 4 карток
    утвориться слово <<мама>>?

    Треба обчислити ймовірність $\P\left(M^{(1)} \cap A^{(2)} \cap M^{(3)} \cap A^{(4)}\right)$,
    де верхній індекс означає крок, на якому витягнуто літеру.
    $\P\left(M^{(1)}\right) = \frac{2}{10}$, $\P\left(A^{(2)} / M^{(1)}\right) = \frac{3}{9}$,
    $\P\left(M^{(3)}/\left(M^{(1)} \cap A^{(2)}\right)\right) = \frac{1}{8}$, $\P\left(A^{(4)} / \left(M^{(1)} \cap A^{(2)} \cap M^{(3)}\right)\right) = \frac{2}{7}$.
    Тому за формулою \eqref{eq:mult_for_n} $\P\left(M^{(1)} \cap A^{(2)} \cap M^{(3)} \cap A^{(4)}\right) = \frac{2 \cdot 3 \cdot 1 \cdot 2}{10 \cdot 9 \cdot 8 \cdot 7} = \frac{1}{420}$.
\end{example}

\noindent\textbf{Теорема додавання.} Нехай $A_1, A_2, ..., A_n \in \mathcal{F}$ --- незалежні у сукупності.
Тоді з рівностей $\P\left(\bigcup\limits_{k=1}^n A_k\right) = 1 - \P\left(\overline{\bigcup\limits_{k=1}^n A_k}\right) = 1 - \P\left(\bigcap\limits_{k=1}^n \overline{A_k}\right)$
випливає формула
\begin{equation}
    \P\left(\bigcup\limits_{k=1}^n A_k\right) = 1 - \prod\limits_{k=1}^n \P(\overline{A_k})
\end{equation}
Зокрема, для двох незалежних подій $A$ і $B$: $\P(A\cup B) = 1 - \P(\overline{A}) \cdot \P(\overline{B})$.

\subsection{Формула повної ймовірності}
Розглядаємо деякий ймовірнісний простір $\left\{ \Omega, \mathcal{F}, \P\right\}$.

Нехай $H_1, H_2, ..., H_n \in \mathcal{F}$ (може бути й зліченна кількість) --- деяка повна група подій,
які називаємо \emph{гіпотезами (припущеннями)}, а подія $A$ відбувається разом з якоюсь гіпотезою. Тоді має місце
\emph{формула повної ймовірності}:
\begin{equation}\label{eq:total_prob}
    \P\left( A \right) = \sum\limits_{k=1}^n \P(H_k)\cdot \P(A/H_k)
\end{equation}
\begin{proof}
    $\P(A) = \P(A \cap \Omega) = \P\left(A \cap \left(\bigcup\limits_{k=1}^n H_k\right)\right) = \P\left(\bigcup\limits_{k=1}^n (A\cap H_k)\right) = \sum\limits_{k=1}^n \P(A\cap H_k) = \sum\limits_{k=1}^n \P(H_k)\cdot \P(A/H_k)$.
\end{proof}
У випадку зліченної кількості гіпотез формула \eqref{eq:total_prob} доводиться аналогічно та перетворюється
на $\P\left( A \right) = \sum\limits_{k=1}^{\infty} \P(H_k)\cdot \P(A/H_k)$.

\begin{example}
    \begin{enumerate}
        \item У магазин постачають $80\%$ телефонів з Китаю, $15\%$ з В'єтнаму та $5\%$ з Кореї,
        причому бракованих відповідно $1\%$, $0.1\%$ та $0.01\%$.
        Знайти ймовірність події $A = \left\{ \text{куплений телефон буде бракованим}\right\}$.

        За умовою введемо гіпотези $H_1 = \left\{ \text{телефон з Китаю}\right\}$,
        $H_2 = \left\{ \text{телефон з В'єтнаму}\right\}$ та $H_3 = \left\{ \text{телефон з Кореї}\right\}$.
        $\P(H_1) = 0.8$, $\P(H_2) = 0.15$, $\P(H_3) = 0.05$,
        $\P(A/H_1) = 0.01$, $\P(A/H_2) = 0.001$, $\P(A/H_3) = 0.0001$.
        Тоді за формулою \eqref{eq:total_prob} $\P(A) = 0.8\cdot 0.01 + 0.15\cdot 0.001 + 0.05\cdot 0.0001 = 0.008155$.
        \item Серед $N$ екзаменаційних білетів $n$ <<щасливих>>, $n<N$.
        У якого студента ймовірність витягнути <<щасливий>> білет більша --- у того, хто тягне першим, чи того, хто тягне другим?

        Позначимо через $A$ і $B$ події, що <<щасливий>> білет витягнув перший та другий студент відповідно. За умовою $\P(A) = \frac{n}{N}$.
        Щоб скористатися формулою \eqref{eq:total_prob} для обчислення $\P(B)$, введемо гіпотези $H_1$ = $A$ та $H_2$ = $\overline{A}$.
        $\P(H_1) = \frac{n}{N}$, $\P(H_2) = \frac{N-n}{N}$.
        \\ $\P(B) = \P(H_1)\cdot \P(B/H_1) + \P(H_2)\cdot \P(B/H_2) = \frac{n}{N}\cdot \frac{n-1}{N-1} + \frac{N-n}{N}\cdot \frac{n}{N-1} = 
        \frac{n^2 - n + N\cdot n - n^2}{N\cdot(N-1)} = \frac{n\cdot (N-1)}{N\cdot(N-1)} = \frac{n}{N}$.
    \end{enumerate}
\end{example}

\subsection{Формула Баєса}
Як і раніше, нехай $H_1, H_2, ..., H_n \in \mathcal{F}$ --- повна група подій деякого СЕ, які називаємо гіпотезами, причому
перед проведенням експерименту відомі їх \emph{апріорні} ймовірності $\P(H_1), \P(H_2), ..., \P(H_n)$.
В результаті проведення експерименту відбулась деяка подія $A$.
Постає питання: чому рівні \emph{апостеріорні} ймовірності $\P(H_1/A), \P(H_2/A), ..., \P(H_n/A)$?
Тобто, чому дорівнюють ймовірності, що мала місце кожна з гіпотез за умови, що подія $A$ відбулась? На це питання дає відповідь \emph{формула Баєса}:
\begin{equation}\label{eq:bayes}
    \P(H_i/A) = \frac{\P(H_i \cap A)}{\P(A)} = \frac{\P(H_i) \cdot \P(A/H_i)}{\sum\limits_{k=1}^n \P(H_k)\cdot \P(A/H_k)}, i = 1,...,n
\end{equation}
% \begin{remark}
%     В найпростішому випадку, коли гіпотези лише дві (деяка $H$ та протилежна їх $\overline{H}$), формулу \eqref{eq:bayes} можна записати так:
%     \begin{gather*}
%         \P(H / A) = \frac{
%             \P(H) \cdot \P(A / H)
%         }{
%             \P(H) \cdot \P(A / H) + \P(\overline{H}) \cdot \P(A / \overline{H})
%         }
%     \end{gather*}
% \end{remark}
\begin{example}
    Нехай на деяку хворобу хворіє 1\% людей. Тестування показує наявність цієї хвороби у 99\% дійсно хворих людей,
    і у 2\% тих, що насправді не хворіють. Яка ймовірність того, що людина дійсно хвора, якщо тест має позитивний результат?
    
    Для застосування формули Баєса \eqref{eq:bayes} введемо гіпотези $H = \left\{\text{людина хвора}\right\}$, $\overline{H} = \left\{\text{людина не хвора}\right\}$ та
    подію $A = \left\{\text{тест дав позитивний результат}\right\}$.
    З умови $\P(H) = 0.01$, $\P(\overline{H}) = 0.99$, $\P(A / H) = 0.99$ та $\P(A / \overline{H}) = 0.02$.
    За формулою \eqref{eq:bayes}:
    $\P(H / A) = \frac{
        \P(H) \cdot \P(A / H)
    }{
        \P(\overline{H}) \cdot \P(A / \overline{H})
    } = \frac{
        0.01 \cdot 0.99
    }{
        0.01 \cdot 0.99 + 0.99 \cdot 0.02
    } = \frac{1}{3}
    $. 
    Отже, лише в третині випадків позитивний результат тесту означає, що людина дійсно хвора. Цей приклад показує,
    що для розуміння того, як добре деяка перевірка визначає деяку рідкісну ознаку, важливо знати не лише 
    ймовірність, з якою ця перевірка підтвердить наявність ознаки, а й те, як часто ця перевірка дає позитивний результат в тих випадках,
    де його насправді немає.
\end{example}
\begin{example}
    В групі з 10 осіб троє вчаться на <<5>>, четверо на <<4>>, двоє на <<3>> та один на <<2>>.
    Викладач підготував на екзамен 20 питань. Студенти, які вчаться на <<5>>, знають відповіді на всі,
    на <<4>> --- на 16, на <<3>> --- на 10, на <<2>> --- на 5.
    Екзаменаційний білет містить 3 питання. Деякий студент відповів правильно на всі 3.
    Яка ймовірність того, що він вчиться на <<2>>?

    Введемо гіпотези $H_1 = \left\{ \text{студент вчиться на <<5>>}\right\}$, $H_2 = \left\{ \text{студент вчиться на <<4>>}\right\}$,
    $H_3 = \left\{ \text{студент вчиться на <<3>>}\right\}$, $H_4 = \left\{ \text{студент вчиться на <<2>>}\right\}$.
    За умовою $\P(H_1) = \frac{3}{10}$, $\P(H_2) = \frac{4}{10}$, $\P(H_3) = \frac{2}{10}$, $\P(H_4) = \frac{1}{10}$. 
    Позначимо $A = \left\{ \text{студент відповів на всі три питання}\right\}$. 
    Тоді $\P(A/H_1) = 1$, $\P(A/H_2) = \frac{16 \cdot 15 \cdot 14}{20 \cdot 19 \cdot 18}$, 
    $\P(A/H_3) = \frac{10 \cdot 9 \cdot 8}{20 \cdot 19 \cdot 18}$, 
    $\P(A/H_4) = \frac{5 \cdot 4 \cdot 3}{20 \cdot 19 \cdot 18}$.
    За формулою \eqref{eq:bayes} шукана ймовірність $\P(H_4/A) = \frac{\P(H_4) \cdot \P(A/H_4)}{\sum\limits_{k=1}^4 \P(H_k)\cdot \P(A/H_k)} =
    \frac{0.1 \cdot 5 \cdot 4 \cdot 3}{0.3 \cdot 20 \cdot 19 \cdot 18 + 0.4 \cdot 16 \cdot 15 \cdot 14 + 0.2 \cdot 10 \cdot 9 \cdot 8 + 0.1 \cdot 5 \cdot 4 \cdot 3}
     = \frac{60}{35460} = \frac{1}{591}$.

\end{example}