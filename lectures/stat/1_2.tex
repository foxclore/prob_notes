% !TEX root = ../../main.tex
\section{Точкові оцінки}
Нехай $\xi$ --- ГС, а $F_{\xi}(x, \theta_1, \theta_2, ..., \theta_k)$ --- її функція розподілу, де
$\theta_1, \theta_2, ..., \theta_k$ --- набір параметрів. Тип самої функції розподілу у випадку точкового оцінювання
вважається відомим, невідомими є параметри: наприклад, $\lambda$ в експоненційному законі чи $a$ та $\sigma$ в нормальному.
\begin{definition}
    \emph{Точковою оцінкою} $\theta^*$ невідомого параметру $\theta$ називається деяка статистика
    $\theta^*(\xi_1, ..., \xi_n)$, значення якої на конкретній
    реалізації вибірки приймається за наближене значення $\theta$. Іноді вводиться позначення $\theta^*_n$,
    щоб вказати залежність оцінки від обсягу вибірки.
\end{definition}
Зрозуміло, що точкова оцінка $\theta^*$, на відміну від параметру $\theta$, є випадковою величиною, 
яка залежить від закону розподілу $\xi$ та обсягу вибірки. Безумовно, можна ввести багато функцій від результатів спостережень, 
які можна брати в якості $\theta^*$. Наприклад, якщо параметр $\theta$ є математичним сподіванням $\xi$, 
то за оцінку математичного сподівання за результатами спостережень можна взяти середнє арифметичне, моду, медіану, 
півсуму найбільшого та найменшого значень вибірки тощо. 
Отже, яку статистику краще обрати? Назвати <<найкращою>> оцінкою ту, яка найбільш близька до істинного значення оцінюваного параметру, 
неможливо, оскільки точкова оцінка --- випадкова величина. Таким чином, робити висновки про якість оцінки варто не по її конкретним значенням, 
а по її розподілу. В зв'язку з цим розглянемо вимоги, що висувають до точкових оцінок.

\subsection{Незміщеність оцінки}
 \begin{definition}
    Точкова оцінка $\theta^*$ параметру $\theta$ називається \emph{незміщеною}, якщо
    \begin{gather}\label{estim_unbiased}
        \E\theta^*_n = \theta \text{ для всіх } n\in\mathbb{N}
    \end{gather} 
    і \emph{асимптотично незміщеною}, якщо $\underset{n\to\infty}{\lim} \E\theta^*_n = \theta$. 
 \end{definition}
 \begin{example} 
    Розглянемо деякі важливі незміщені оцінки.
    \begin{enumerate}
        \item \emph{Вибіркове середнє} --- незміщена оцінка математичного сподівання:
        $\overline{\xi} = \frac{1}{n}\sum\limits_{k=1}^n \xi_k$, $\E\overline{\xi} = \frac{1}{n}\sum\limits_{k=1}^n \E\xi_k = \frac{1}{n} \cdot{n} \cdot{\E\xi} = \E\xi$,
        оскільки всі $\xi_k$ однаково розподілені.
        \item \emph{Вибіркова дисперсія} --- незміщена оцінка дисперсії у випадку відомого математичного сподівання $\E\xi$:
        $\D^*\xi = \frac{1}{n}\sum\limits_{k=1}^n \left(\xi_k - \E\xi \right)^2$, 
        $\E\left( \D^* \xi\right) = \frac{1}{n}\sum\limits_{k=1}^n \E\left(\xi_k - \E\xi \right)^2 = \D\xi$ знову через однаковий розподіл $\xi_k$.
        \item Дослідимо вибіркову дисперсію, але у випадку невідомого математичного сподівання. 
        \begin{gather*}
            \D^*\xi = \frac{1}{n}\sum\limits_{k=1}^n \left(\xi_k - \overline{\xi} \right)^2 = 
            \frac{1}{n}\sum\limits_{k=1}^n \left((\xi_k - \E\xi) - (\overline{\xi} - \E\xi) \right)^2 =  \\
            = \frac{1}{n}\sum\limits_{k=1}^n \left(\xi_k - \E\xi \right)^2 - 2\left(\overline{\xi} - \E\xi\right)\cdot 
            \underbrace{\frac{1}{n}\sum\limits_{k=1}^n \left(\xi_k - \E\xi\right)}_{\overline{\xi} - \E\xi} + \left(\overline{\xi} - \E\xi\right)^2 = \\
            = \frac{1}{n}\sum\limits_{k=1}^n \left(\xi_k - \E\xi \right)^2 - \left(\overline{\xi} - \E\xi\right)^2
        \end{gather*}
        Перший доданок --- це формула для вибіркової дисперсії при відомому математичному сподіванні, а 
        $\E\xi = \E\overline{\xi}$, тому
        \begin{gather*}
            \E\left( \D^* \xi\right) = \D\xi - \E\left(\overline{\xi} - \E\xi\right)^2= \D\xi - \D\overline{\xi} \\
            \D\overline{\xi} = \D\left(\frac{1}{n}\sum\limits_{k=1}^n \xi_k \right) = \frac{1}{n^2} \sum\limits_{k=1}^n \D\xi_k = \frac{1}{n} \D\xi
        \end{gather*}
        Дві останні рівності одержані через незалежність та однаковий розподіл $\xi_k$. Отже, маємо $\E\left( \D^* \xi\right) = \left( 1- \frac{1}{n}\right) \D\xi$,
        тому ця оцінка є лише асимптотично незміщеною. Проте, оцінка $\D^{**}\xi = \frac{n}{n-1} \D^*\xi$ буде незміщеною.
        Таким чином, якщо $\E\xi$ невідоме, то \emph{виправлена вибіркова дисперсія} 
        $\D^{**}\xi = \frac{1}{n-1} \sum\limits_{k=1}^n \left(\xi_k - \overline{\xi} \right)^2$ є незміщеною оцінкою дисперсії.
        \item Нехай $\xi \sim \mathrm{U}\left< a; b\right>$, перевіримо незміщеність $a^*_n = \underset{1\leq k \leq n}{\min}\xi_k$.
        Знайдемо $\E a^*_n$. Як відомо, 
        $f_{\min}(x) = n \left(1-F_{\xi}(x)\right)^{n-1} f_{\xi}(x) = n\left( 1- \frac{x-a}{b-a}\right)^{n-1}\frac{1}{b-a} = n\cdot\frac{(b-x)^{n-1}}{(b-a)^n}$, якщо
        $x \in \left< a; b\right>$, та $0$ інакше.
        \begin{gather*}
            \E a^*_n = \frac{n}{(b-a)^n} \int_a^b x(b-x)^{n-1} dx = \left[ b-x = t \right] = 
            \frac{n}{(b-a)^n} \int_0^{b-a} (b-t)t^{n-1} dt = \\
            = \frac{n}{(b-a)^n} \left.\left( \frac{bt^n}{n} - \frac{t^{n+1}}{n+1}\right)\right|_0^{b-a} = 
            \frac{n}{(b-a)^n} \left(\frac{b(b-a)^n}{n} - \frac{(b-a)^{n+1}}{n+1}\right) = \\
            = b - \frac{n(b-a)}{n+1} = \frac{bn + b - nb + na}{n+1} = a \cdot \frac{n}{n+1} + \frac{b}{n+1} \neq a
         \end{gather*}
         Але $\underset{n\to\infty}{\lim} \E a^*_n = a$, тому $a^*_n$ є асимптотично незміщеною оцінкою.
    \end{enumerate}
 \end{example}
 \begin{exercise}
     Перевірити, що $b^*_n = \underset{1\leq k \leq n}{\max}\xi_k$ є асимптотично незміщеною оцінкою для параметра $b$ у випадку $\xi \sim \mathrm{U}\left< a; b\right>$.
     Користуючись вже дослідженими оцінками $a^*_n$ та $b^*_n$, знайти незміщені оцінки для параметрів $a$ і $b$ (підказка: це будуть деякі лінійні комбінації $a^*_n$ та $b^*_n$).
 \end{exercise}

\subsection{Конзистентність оцінки}
\begin{definition}
    Точкова оцінка $\theta^*$ параметру $\theta$ називається \emph{конзистентною}, якщо
    \begin{gather}\label{estim_consis}
        \forall \; \varepsilon > 0: \underset{n \to \infty}{\lim} \P\left\{|\theta^*_n - \theta| \geq \varepsilon\right\}= 0
        \Longleftrightarrow \theta^*_n \overset{\mathrm{P}}{\longrightarrow} \theta, n \to \infty
    \end{gather}
\end{definition}
Конзистентність оцінки можна перевіряти за означенням. Але для незміщених та асимптотично незміщених оцінок є
\emph{достатня умова} конзистентності.
\begin{proposition*}
    Якщо $\theta^*_n$ --- незміщена чи асимптотично незміщена оцінка та $\D\theta^*_n \to 0$, $n \to \infty$,
    то ця оцінка є конзистентною.
\end{proposition*}
\begin{proof}
    Якщо $\theta^*_n$ незміщена, то 
    $\P\left\{|\theta^*_n - \theta| \geq \varepsilon\right\}=\P\left\{|\theta^*_n - \E\theta^*_n| \geq \varepsilon\right\} 
    \leq \frac{\D\theta^*_n}{\varepsilon^2} \to 0, n\to \infty$.
    Якщо $\theta^*_n$ асимптотично незміщена, то $\underset{n\to\infty}{\lim} \E\theta^*_n = \theta$,
    звідки за критерієм збіжності в середньому квадратичному до константи маємо 
    $\theta^*_n \overset{\text{СК}}{\longrightarrow} \theta \Rightarrow \theta^*_n \overset{\mathrm{P}}{\longrightarrow} \theta, n\to\infty$.
\end{proof}
\begin{remark}
    Також є поняття \emph{сильно конзистентної оцінки}, де збіжність за ймовірністю замінюється збіжністю майже напевно.
    Такі оцінки надалі розглядати не будемо через складність дослідження такої збіжності в загальному випадку. 
\end{remark}
\begin{example}
    Перевіримо конзистентність оцінок, незміщеність чи асимптотичну незміщеність яких вже дослідили.
    \begin{enumerate}
        \item \emph{Вибіркове середнє} $\overline{\xi} = \frac{1}{n}\sum\limits_{k=1}^n \xi_k$ є конзистентною оцінкою
        за законом великих чисел (і навіть сильно конзистентною).
        \item \emph{Вибіркова дисперсія} $\D^*\xi = \frac{1}{n}\sum\limits_{k=1}^n \left(\xi_k - \E\xi \right)^2$
        є незміщеною оцінкою дисперсії в разі відомого $\E\xi$. 
        $\D\left(\D^*\xi\right) = \frac{1}{n^2}\sum\limits_{k=1}^n \D\left(\xi_k - \E\xi \right)^2 =
        \frac{1}{n} \D\left(\xi - \E\xi \right)^2 \to 0, n\to\infty$, тому ця оцінка конзистентна.
        \item \emph{Виправлена вибіркова дисперсія} $\D^{**}\xi = \frac{n}{n-1} \D^*\xi$ є незміщеною оцінкою дисперсії в разі невідомого $\E\xi$. 
        $\D\left(\D^{**}\xi\right) = \left(\frac{n}{n-1} \right)^2 \cdot \D\left(\D^*\xi\right) \to 0, n\to\infty$, тому ця оцінка конзистентна.
        \item Нехай $\xi \sim \mathrm{U}\left< a; b\right>$, $a^*_n = \underset{1\leq k \leq n}{\min}\xi_k$ --- асимптотично незміщена оцінка $a$.
        Перевірятимемо конзистентність за означенням:
        \begin{gather*}
            \varepsilon > 0, \; \P\left\{|a^*_n - a| < \varepsilon\right\} = 
            \P\left\{a - \varepsilon < a^*_n < a + \varepsilon\right\} = \int_{a}^{a+\varepsilon} f_{\min}(x) dx = \\
            = \frac{n}{(b-a)^n}\int_{a}^{a+\varepsilon} (b-x)^{n-1} dx =
            -\frac{n}{(b-a)^n} \cdot \left. \frac{(b-x)^n}{n} \right|_{a}^{a+\varepsilon} = \\
            = \frac{1}{(b-a)^n} \cdot \left( (b-a)^n  - (b-a-\varepsilon)^n\right) = 
            1 - \left(1 - \frac{\varepsilon}{b-a}\right)^n \to 1, n\to\infty
        \end{gather*} 
        Отже, оцінка є конзистентною.
    \end{enumerate}
\end{example}
\begin{exercise}
    Перевірити конзистентність оцінки $b^*_n = \underset{1\leq k \leq n}{\max}\xi_k$ для $\xi \sim \mathrm{U}\left< a; b\right>$.
\end{exercise}
\begin{remark}
    Конзистентність та незміщеність вибіркового середнього, виправленої та звичайної вибіркових дисперсій було встановлено для будь-якого розподілу $\xi$.
\end{remark}

\subsection{Ефективність оцінки}
Основне питання задачі оцінювання параметрів розподілу --- наскільки великою є похибка. 
Введені означення незміщеної та конзистентної оцінки показують, відповідно, чи правильні в середньому значення цієї оцінки,
та чи покращується точність оцінювання зі збільшенням обсягу вибірки.
Зрозуміло, що для оцінювання параметру $\theta$ можна запропонувати декілька незміщених оцінок, значення яких за визначенням
зосереджені навколо справжнього значення $\theta$. Природно вимагати від <<найкращої>> такої оцінки найменшої можливої дисперсії.
\begin{definition}
    Нехай $\Theta_n$ --- множина усіх незміщених оцінок параметру $\theta$ за вибірками фіксованого обсягу $n$. 
    Оцінка $\theta^*_{\text{еф}}$ називається
    \emph{ефективною}, якщо 
    \begin{gather}\label{estim_eff}
        \D\theta^*_{\text{еф}} = \underset{\theta^* \in \Theta_n}{\inf} \D\theta^*
    \end{gather}
\end{definition}
Означення ефективності не надто сприяє дослідженню оцінки. По-перше, не завжди
легко обчислити дисперсію оцінки. По-друге, навіть якщо її вдасться обчислити, то немає гарантії, що ця дисперсія буде найменшою серед дисперсій усіх оцінок
з $\Theta_n$. Перевіряти ефективність оцінок допомагає нерівність Рао-Крамера.

\begin{theorem*}[нерівність Рао-Крамера]
    Якщо $\theta^*$ --- незміщена оцінка, то $\D\theta^* \geq \frac{1}{\mathcal{I}(\theta)}$, 
    де $\mathcal{I}(\theta) = \E\left( \frac{\partial \ln \mathcal{L}(\vec{\xi}, \theta)}{\partial \theta}\right)^2$ --- 
    {<<кількість інформації за Фішером>>}. Для ефективної оцінки досягається рівність. Тут
    $\mathcal{L}(\vec{\xi}, \theta)$ --- функція правдоподібності, що залежить від випадкової вибірки $\vec{\xi}$ та 
    невідомого параметру $\theta$. 
\end{theorem*}
\begin{proof}
    Доведемо цю нерівність у випадку неперервної ГС. 
    За припущенням про незміщеність $$\E\theta^* = \int_{\mathbb{R}^n}\theta^*(\vec{x}) f_{\vec{\xi}}(\vec{x}, \theta) d\vec{x} = \theta$$
    З іншого боку, $$ \theta = \theta \cdot 1 = \theta \cdot \int_{\mathbb{R}^n}f_{\vec{\xi}}(\vec{x}, \theta) d\vec{x}$$
    Отже, маємо рівність, яку продиференціюємо по $\theta$: 
    \begin{gather*}
        \int_{\mathbb{R}^n}(\theta^*(\vec{x}) - \theta) f_{\vec{\xi}}(\vec{x}, \theta) d\vec{x} = 0 \\
        \frac{\partial}{\partial  \theta}\left(\int_{\mathbb{R}^n}(\theta^*(\vec{x}) - \theta) f_{\vec{\xi}}(\vec{x}, \theta) d\vec{x}\right) =
         -\underbrace{\int_{\mathbb{R}^n}f_{\vec{\xi}}(\vec{x}, \theta) d\vec{x} }_1
         + \int_{\mathbb{R}^n}(\theta^*(\vec{x}) -\theta) \frac{\partial f_{\vec{\xi}}(\vec{x}, \theta)}{\partial \theta} d\vec{x} = 0 \\
         \int_{\mathbb{R}^n}(\theta^*(\vec{x}) -\theta) \frac{\partial f_{\vec{\xi}}(\vec{x}, \theta)}{\partial \theta} d\vec{x} = 
         \int_{\mathbb{R}^n}(\theta^*(\vec{x}) -\theta) \frac{\partial f_{\vec{\xi}}(\vec{x}, \theta)}{\partial \theta} \frac{f_{\vec{\xi}}(\vec{x}, \theta)}{f_{\vec{\xi}}(\vec{x}, \theta)} d\vec{x} = 1
    \end{gather*}
    Оскільки $\frac{\partial f_{\vec{\xi}}(\vec{x}, \theta)}{\partial \theta} \frac{1}{f_{\vec{\xi}}(\vec{x}, \theta)} = \frac{\partial\ln f_{\vec{\xi}}(\vec{x}, \theta)}{\partial \theta}$,
    а у випадку неперервної ГС $\mathcal{L}(\vec{\xi}, \theta) = \prod\limits_{k=1}^n f_{\xi}(\xi_k, \theta) = f_{\vec{\xi}}(\vec{\xi}, \theta)$, останню рівність можна записати у вигляді 
    $1 = \E\eta_1 \eta_2$, де $\eta_1 = \theta^*(\vec{\xi}) -\theta$, $\eta_2 = \frac{\partial\ln \mathcal{L}(\vec{\xi}, \theta)}{\partial \theta}$.
    Оскільки $\left( \E\eta_1 \eta_2 \right)^2 \leq \E\eta_1^2 \cdot \E\eta_2^2$, маємо $1 \leq \D\theta^* \cdot \mathcal{I}(\theta)$, що і треба було довести.
\end{proof}
\noindent\textbf{Наслідок.} В нерівності $\left( \E\eta_1 \eta_2 \right)^2 \leq \E\eta_1^2 \cdot \E\eta_2^2$ рівність досягається
тоді і тільки тоді, коли $\eta_1$ та $\eta_2$ лінійно залежні. Оскільки для ефективної оцінки досягається рівність, то маємо такий 
\emph{критерій ефективності незміщеної оцінки}: незміщена оцінка $\theta^*$ є оптимальною тоді й тільки тоді, коли
\begin{gather}
    \frac{\partial\ln \mathcal{L}(\vec{\xi}, \theta)}{\partial \theta} = C(n, \theta) \cdot\left(\theta^*(\vec{\xi}) - \theta\right)
\end{gather}
Тут $C(n, \theta)$ --- деяке значення, що залежить лише від $n$ та $\theta$.
\begin{remark}
    В умові теореми необхідно накладати деякі умови, виконання яких припускалися при доведенні: додатність та 
    диференційовність функції правдоподібності на області визначення,
    існування (скінченність) $\mathcal{I}(\theta)$ та можливість диференціювання відповідного інтегралу по параметру.
\end{remark}
\begin{example}
    Нехай ГС $\xi \sim \mathrm{Poiss}(a)$. $\E\xi = a$, тому $\overline{\xi} = \frac{1}{n}\sum\limits_{k=1}^n \xi_k$ --- 
    незміщена оцінка $a$. Перевіримо її ефективність.
    \begin{gather*}
        \ln \mathcal{L}(\vec{x}, a) = -n a + \ln a \cdot \sum\limits_{k=1}^n x_k - \sum\limits_{k=1}^n \ln{x_k!} \\
        \frac{\partial  \ln \mathcal{L}(\vec{x}, a)}{\partial a} = -n + \frac{1}{a} \cdot \sum\limits_{k=1}^n x_k = 
        -n + \frac{n}{a} \cdot \overline{x} = \frac{n}{a} \cdot(\overline{x} - a)
    \end{gather*}
    Перейдемо до випадкової вибірки: $\frac{\partial  \ln \mathcal{L}(\vec{\xi}, a)}{\partial a} = C(n, a) \cdot(a^* - a)$.
    Отже, ця оцінка є ефективною.
\end{example}

Розглянемо ще одну важливу теорему, що стосується ефективних оцінок.
\begin{theorem*}[єдиність ефективної незміщеної точкової оцінки]
    Якщо $\theta^* = \theta^* (\vec{\xi})$ --- ефективна незміщена оцінка, то вона єдина.
\end{theorem*}
\begin{proof}
    Нехай $\theta_1^*$ --- інша ефективна незміщена оцінка, $\E\theta^* = \E\theta_1^* = \theta$,
    $\D\theta^* = \D\theta_1^* = \sigma^2$. Розглянемо оцінку $\theta_2^* = \frac{\theta^* + \theta_1^*}{2}$, яка теж є незміщеною.
    $\D\theta_2^* = \frac{1}{4}\left(\D\theta^* + \D\theta_1^* + 2{cov}(\theta^*, \theta_1^*) \right) = \frac{1}{2}\left( \sigma^2 + {cov}(\theta^*, \theta_1^*)\right)$.
    $\left|{cov}(\theta^*, \theta_1^*) \right| \leq \sqrt{\D\theta^* \cdot \D\theta_1^*} = \sigma^2$.
    Отже, $\D\theta_2^* \leq \sigma^2$. Але за умовою дисперсія ефективної $\theta$ рівна $\sigma^2$, тому $\D\theta_2^* = \sigma^2$.
    Це означає, що в ${cov}(\theta^*, \theta_1^*) \leq \sigma^2$ досягається рівність, тому $\theta^*$ та $\theta_1^*$ є лінійно залежними, тобто
    $\theta_1^* = k\cdot \theta^* + b$. Тепер $\theta = \E\theta_1^* = k\cdot \E\theta^* + b = k\cdot \theta + b$, звідки $b = (1-k)\theta$.
    $\theta_1^* = k\cdot \theta^* + (1-k)\theta \Leftrightarrow \theta_1^* - \theta = k \cdot \left(\theta^* - \theta \right)$.
    Скориставшись ${cov}(\theta^*, \theta_1^*) = \sigma^2$, отримаємо 
    $\sigma^2 = \E\left(\theta^* - \theta \right)\left(\theta_1^* - \theta \right) = k \cdot \E\left(\theta^* - \theta \right)^2 = k \cdot \sigma^2$,
    звідки $k = 1$, тому $b = 0$ і $\theta^*_1 = \theta^*$.
\end{proof}

Також вводитися поняття \emph{асимптотично ефективної оцінки.} Це означає, що 
$\D\theta^* \cdot \mathcal{I}(\theta) \to 1$ при $n\to\infty$. Оскільки за нерівністю Рао-Крамера для незміщених оцінок
виконується $\D\theta^* \cdot \mathcal{I}(\theta) \leq 1$, то, якщо треба порівняти декілька незміщених оцінок,
 які не є ефективними, кращою обирається та, у якої добуток $\D\theta^* \cdot \mathcal{I}(\theta)$ більше.

\subsection{Асимптотична нормальність оцінки}
\begin{definition}
    Оцінка $\theta_n^* = \theta^* (\xi_1, ..., \xi_n)$ називається \emph{асимптотично нормальною}, якщо 
    \begin{gather}
        \frac{\theta_n^* - \theta}{\sqrt{\D\theta_n^*}} \overset{\mathrm{F}}{\longrightarrow} \eta \sim \mathrm{N}(0, 1), \; n \to \infty
    \end{gather}
\end{definition}
Це поняття в подальшому дозволить наближувати розподіл точкових оцінок нормальним розподілом.
Наприклад, для вибіркового середнього $\overline{\xi} = \frac{1}{n} \sum\limits_{k=1}^n \xi_k$ згідно ЦГТ маємо
$$ 
\frac{\overline{\xi} - \E\xi}{\sqrt{\D\overline{\xi}}} = \frac{\frac{1}{n} \sum\limits_{k=1}^n (\xi_k  -\E\xi)}{\sqrt{\D\xi/n}} = 
\frac{1}{\sqrt{n}} \sum\limits_{k=1}^n \left(\frac{\xi_k - \E\xi}{\sqrt{\D\xi}} \right)
\overset{\mathrm{F}}{\longrightarrow} \eta \sim \mathrm{N}(0, 1), \; n \to \infty
$$

Таким чином, при великих $n$ можна вважати розподіл $\overline{\xi}$ приблизно рівним $ \mathrm{N} \left(\E\xi, \frac{\D\xi}{n}\right)$.

\section{Методи отримання точкових оцінок}
Розглянемо  методи, які допомагають знаходити точкові оцінки параметрів розподілу ГС.
\subsection{Метод моментів}
\emph{Метод моментів} базується на тому, що часто деякі моменти $\xi$ функціонально залежать від параметрів розподілу.
В такому випадку, в разі <<хорошої>> залежності можна виразити параметри розподілу як функції від моментів і отримати точкові оцінки параметрів,
замінивши в отриманих вираз моменти на їх вибіркові аналоги.
\begin{example}
    Знайдемо декілька оцінок методом моментів.
    \begin{enumerate}
        \item $\xi \sim \mathrm{U}\left< a; b\right>$. Відомо, що $\E \xi = \frac{a+b}{2}$, але щоб виразити через моменти обидва параметри, необхідне ще одне рівняння:
        $\D \xi = \frac{(b-a)^2}{12}$. Отримали систему:
        $$ \begin{cases}
            a^* + b^* = 2\overline{\xi} \\
            b^* - a^* = 2\sqrt{3 \D^{**} \xi}
        \end{cases} \Rightarrow
        \begin{cases}
            b^* = \overline{\xi} + \sqrt{3 \D^{**} \xi} \\
            a^* = \overline{\xi} - \sqrt{3 \D^{**} \xi}
        \end{cases}$$
        \item Нехай тепер $\xi \sim \mathrm{U}\left< -a; a\right>$. Оскільки $\E\xi = 0$, скористаємося моментом $\E\xi^2 = \frac{a^2}{3}$. В такому випадку
        отримаємо оцінку $a^* = \sqrt{3 \E^{*}\xi^2}$.
        \item $\xi \sim \mathrm{Bin}(N, p)$. Відомо, що $\E \xi = Np$, $\D \xi = Np(1-p)$. В цьому випадку маємо систему 
        $$\begin{cases}
            N^* p^* = \overline{\xi} \\
            N^* p^* (1-p^*) = \D^{**} \xi 
        \end{cases} \Rightarrow
        \begin{cases}
            N^* = \frac{(\overline{\xi})^2}{\overline{\xi} - \D^{**}\xi} \\
            p^* = \frac{\overline{\xi}}{N^*} = \frac{\overline{\xi} - \D^{**}\xi}{\overline{\xi}}
        \end{cases}$$
        Незважаючи на формально правильний результат, застосовувати ці оцінки недоречно: немає жодної гарантії, що $N^*$ прийматиме лише натуральні значення.
        Але якщо параметр $N$ відомий, то можна користуватися оцінкою $p^* = \frac{\overline{\xi}}{N}$.
        \item $\xi \sim \mathrm{Exp}(\lambda)$. Оскільки $\E \xi = \frac{1}{\lambda}$ і $\D \xi = \frac{1}{\lambda^2}$, за допомогою методу моментів можна отримати дві оцінки $\lambda^*$:
        $\lambda_1^* = 1/{\overline{\xi}}$ та $\lambda_2^* = 1/{\sqrt{\D^{**}\xi}}$. В такому випадку порівнюють властивості отриманих оцінок та обирають кращу.
    \end{enumerate}
\end{example}
\begin{exercise}
    Перевірити, що оцінка $\lambda_1^*$ є асимптотично незміщеною. Чи можна зробити з неї незміщену оцінку, і, якщо так, що можна сказати про її ефективність?
\end{exercise}
Як видно з цих прикладів, розподіл оцінок, отриманих за допомогою методу моментів, може бути досить складним. Це, в свою чергу, ускладнює дослідження таких оцінок.
Єдина характеристика, з якою майже не виникає складнощів --- конзистентність. Оскільки оцінки з методу моментів є функціями від вибіркових моментів, які є
конзистентними, то за властивостями збіжності за ймовірністю оцінки також є конзистентними.
\begin{remark}
    В подальшому будемо позначати оцінки, отримані за допомогою методу моментів, через $\theta^*_{\text{ММ}}$, якщо це матиме значення.
\end{remark}
\subsection{Метод максимальної правдоподібності}
\emph{Метод максимальної правдоподібності} полягає в знаходженні таких оцінок невідомих параметрів розподілу ГС, що максимізують значення функції правдоподібності.
Тобто, в якості оцінки $\theta^*$ береться значення $\theta^*(\xi_1, ..., \xi_n) = \underset{\theta}{\mathrm{argmax}} \;{\mathcal{L}(\vec{\xi}, \theta)}$, або ж, у випадку
двох або більше параметрів, $\underset{\theta_1, ..., \theta_k}{\mathrm{argmax}} \; {\mathcal{L}(\vec{\xi}, \theta_1, ..., \theta_k)}$. Оскільки логарифм --- монотонна функція,
то $\ln {\mathcal{L}(\vec{\xi}, \theta_1, ..., \theta_k)}$ досягає максимуму по $\theta_1, ..., \theta_k$ одночасно з ${\mathcal{L}(\vec{\xi}, \theta_1, ..., \theta_k)}$.
Таким чином, в методі максимальної правдоподібності шукають $\underset{\theta_1, ..., \theta_k}{\mathrm{argmax}} \ln{\mathcal{L}(\vec{\xi}, \theta_1, ..., \theta_k)}$.

Надалі для спрощення сприйняття випадкову вибірку в аргументі функції правдоподібності замінимо на реалізацію вибірки. В такому випадку спочатку отримаємо точки максимуму,
що залежать від $x_1, x_2, ..., x_n$ (і це фактично буде значенням оцінки), а саму оцінку, як функцію від випадкової вибірки, отримаємо зворотною заміною $x_k$ на $\xi_k$.
\begin{example}
    Знайдемо декілька оцінок методом максимальної правдоподібності.
    \begin{enumerate}
        \item $\xi \sim \mathrm{N}(a, \sigma^2)$. Відомо, що $\ln \mathcal{L}(\vec{x}, a, \sigma^2) = -\frac{n}{2}\ln{2\pi} - \frac{n}{2} \ln{\sigma^2} - \frac{1}{2\sigma^2}\sum\limits_{k=1}^n (x_k - a)^2$. 
        Позначимо для зручності $s = \sigma^2$ та шукатимемо максимум функції двох змінних:
        $$\begin{cases}
            \frac{\partial \ln \mathcal{L}}{\partial a} = \frac{1}{s} \sum\limits_{k=1}^n (x_k - a) = \frac{n}{s}(\overline{x} - a) = 0 \\
            \frac{\partial \ln \mathcal{L}}{\partial s} = -\frac{n}{2s} + \frac{1}{2s^2} \sum\limits_{k=1}^n (x_k - \overline{x})^2 = 0
        \end{cases} \Rightarrow
        \begin{cases}
            a^* = \overline{x} \\
            s^* =  \frac{1}{n} \sum\limits_{k=1}^n (\xi_k - a)^2 = (\D^* \xi)_{\text{зн}}
        \end{cases}$$
        $$\begin{pmatrix}
            \frac{\partial^2 \ln \mathcal{L}}{\partial a^2} & \frac{\partial^2 \ln \mathcal{L}}{\partial a \partial s} \\
            \frac{\partial^2 \ln \mathcal{L}}{\partial a \partial s} & \frac{\partial^2 \ln \mathcal{L}}{\partial s^2}
        \end{pmatrix} = 
        \begin{pmatrix}
            -\frac{n}{s} & -\frac{n}{s^2}(\overline{x} - a) \\
            -\frac{n}{s^2}(\overline{x} - a) & \frac{n}{2s^2} - \frac{1}{s^3}\sum\limits_{k=1}^n (x_k - a)^2
        \end{pmatrix}$$
        При $a = a^*$ та $s = s^*$ матриця других похідних дорівнює 
        $$\begin{pmatrix}
            -\frac{n}{s^*} & 0 \\
            0 & \frac{n}{(s^*)^3} \left( 2s^* - \frac{1}{n}\sum\limits_{k=1}^n (x_k - a^*)^2\right)
        \end{pmatrix} =
        \begin{pmatrix}
            -\frac{n}{s^*}  & 0 \\
            0 & \frac{n}{(s^*)^3}\cdot s^*
        \end{pmatrix} = 
        \begin{pmatrix}
            -\frac{n}{s^*}  & 0 \\
            0 & \frac{n}{(s^*)^2}
        \end{pmatrix}$$
        Ця матриця є від'ємно визначеною, тому $\ln \mathcal{L}(\vec{x}, a, \sigma^2)$ дійсно досягає максимуму при
        $a = \overline{x}$ та $\sigma^2 = (\D^* \xi)_{\text{зн}}$. Отже, отримали оцінки $a^* = \overline{\xi}$ та $(\sigma^2)^* = \D^*\xi$.
        \item $\xi \sim \mathrm{Exp}(\lambda, b)$, вважатимемо $\lambda$ відомим.
        $ \ln \mathcal{L}(\vec{x}, b) = n \ln \lambda - \lambda\left(\sum\limits_{k=1}^n x_k - n b\right)$, але сама функція правдоподібності приймає нульові значення при $x_k < b$.
        Якщо просто продиференціювати функцію правдоподібності, отримаємо 
        $\frac{\partial \ln \mathcal{L}(\vec{x}, b)}{\partial b} = \lambda n$ --- цей вираз завжди більше $0$ та не залежить від вибірки. Отже, 
        $\ln \mathcal{L}(\vec{x}, b)$ монотонно зростає відносно $b$. За змістом самого параметру $x_k \geq b$ для всіх $k = 1, ..., n$. Таким чином,
        максимальне можливе значення $b$ --- це $\underset{1\leq k \leq n}{\min}x_k$. Отже, $b^* = \underset{1\leq k \leq n}{\min}\xi_k$.
        \item $\xi \sim \mathrm{U}[\theta; \theta+1]$, $\theta \in \mathbb{R}$. Спочатку запишемо щільність:
        $f_{\xi}(x) = \begin{cases}
            1, & x\in[\theta; \theta+1] \\
            0, & x\notin[\theta; \theta+1]
        \end{cases}$. Отже, функція правдоподібності має вигляд 
        $\mathcal{L}(\vec{x}, \theta) = \begin{cases}
            1, & x_k \in[\theta; \theta+1], k =1, ..., n \\
            0, & \text{інакше}
        \end{cases} = 
        \begin{cases}
            1, & \theta \leq \underset{1\leq k \leq n}{\min}x_k \leq \underset{1\leq k \leq n}{\max}x_k  \leq \theta +1 \\
            0, & \text{інакше}
        \end{cases} = 
        \begin{cases}
            1, & \underset{1\leq k \leq n}{\max}x_k -1 \leq \theta \leq \underset{1\leq k \leq n}{\min}x_k \\
            0, & \text{інакше}
        \end{cases}$. Вона досягає максимуму по $\theta$ в кожній точці з відрізка 
        $\left[\underset{1\leq k \leq n}{\max}x_k -1; \underset{1\leq k \leq n}{\min}x_k\right]$. Таким чином, отримуємо незліченну множину
        оцінок $\theta^*_{\alpha} = (1-\alpha)\cdot \left(\underset{1\leq k \leq n}{\max}\xi_k -1\right) + \alpha\cdot\underset{1\leq k \leq n}{\min}\xi_k$, де
        $\alpha \in [0;1]$.
    \end{enumerate}
\end{example}
\begin{exercise} 
    \begin{enumerate}
        \item Знайти значення $\alpha$, за якого $\theta_{\alpha}^*$ з останнього прикладу буде незміщеною.
        \item Показати, що оцінки $a^* = \underset{1\leq k \leq n}{\min}\xi_k$ та $b^* = \underset{1\leq k \leq n}{\max}\xi_k$ для $\xi \sim \mathrm{U}\left< a; b\right>$
        можна знайти за допомогою методу максимальної правдоподібності.
    \end{enumerate}
\end{exercise}

З розглянутих прикладів видно, що на відміну від методу моментів, метод максимальної правдоподібності може дати оцінки не тільки у вигляді функцій від емпіричних моментів,
а кількість оцінок може бути навіть незліченною. Іноді застосовування методу максимальної правдоподібності є досить складним: наприклад, якщо розглядати $\xi \sim \mathrm{Bin}(N,p)$
з обома невідомими параметрами, то треба буде шукати максимум функції правдоподібності за параметром $N$, що приймає лише скінченну кількість значень та
входить в саму функцію у складі біноміальних коефіцієнтів.
\begin{remark}
    В подальшому будемо позначати оцінки, отримані за допомогою методу максимальної правдоподібності, через $\theta^*_{\text{ММП}}$, якщо це матиме значення.
\end{remark}