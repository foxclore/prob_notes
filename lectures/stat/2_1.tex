% !TEX root = ../../main.tex
\section{Поняття статистичних гіпотез та їх перевірки}
\subsection{Статистичні гіпотези. Помилки першого та другого роду}
Інформація, що отримана при обробці вибірки з деякої генеральної 
сукупності, може бути використана для отримання висновків про всю 
генеральну сукупність. Подібні висновки називають \emph{статистичними}. 
Завдяки їх ймовірнісному характеру завжди можна знайти ймовірність того, 
що прийняте рішення буде помилковим, тобто оцінити ризик того чи 
іншого прийнятого рішення.
\begin{definition}
    \emph{Статистичною непараметричною гіпотезою} $H$
    називається припущення про вигляд розподілу генеральної сукупності, яке 
    перевіряється за вибіркою. Часто розподіл генеральної сукупності відомий і за вибіркою треба 
    перевірити припущення щодо значень параметрів цього розподілу. Такі 
    статистичні гіпотези називають \emph{параметричними}. 
\end{definition}
Гіпотези бувають прості та складні. Гіпотеза називається \emph{простою}, 
якщо вона однозначно визначає розподіл генеральної сукупності, в 
іншому випадку гіпотеза називається \emph{складною}. Наприклад, простою 
гіпотезою є припущення, що ГС має нормальний розподіл з параметрами $0$ та $1$. 
Якщо висувається гіпотеза про те, що ГС має нормальний розподіл з параметрами $1$ та 
$\sigma$, де $\sigma \in (1, 2)$, то ця гіпотеза є складною. Теж саме стосується гіпотез про значення невідомого параметру розподілу ГС:
гіпотеза виду $H: \theta = 1$ є простою, а $H: \theta \in (1, 2)$ --- складною.
\begin{definition}
    Гіпотеза, що перевіряється, називається \emph{нульовою (основною) гіпотезою} й позначається $H_0$.
    Решта гіпотез називається \emph{альтернативними (конкуруючими)} відносно нульової гіпотези й позначаються
    $H_1$, $H_2$ тощо.
\end{definition}
Наприклад, якщо перевіряється проста гіпотеза про рівність параметра $\theta$ деякому значенню $\theta_0$,
тобто, $H_0 : \theta = \theta_0$, то в якості альтернативної гіпотези 
можна розглядати одну з таких: $H_1: \theta \neq \theta_0$, $H_2: \theta > \theta_0$,
$H_3: \theta < \theta_0$, $H_4: \theta = \theta_1$ (де $\theta_1 \neq \theta_0$).

Зазначимо, що математична статистика не дає ніяких рекомендацій
щодо вибору нульової та альтернативної гіпотези, Цей вибір повністю 
визначається дослідником і залежить від поставленої задачі.
\begin{definition}
    Правило, за яким приймається чи відхиляється гіпотеза на 
    основі вибірки, називається \emph{статистичним критерієм} для перевірки 
    гіпотези $H$. Якщо перевіряється гіпотеза про належність розподілу 
    генеральної сукупності до якогось класу розподілів (нормального, 
    рівномірного, Пуассона тощо), то зазначене правило називається \emph{критерієм згоди}.
\end{definition}
Оскільки висновок щодо прийняття або відхилення гіпотези 
приймається за реалізацією вибірки, то вибране рішення може бути 
помилковим. Розрізняють типи таких помилок.

\begin{definition}
    Помилка, яка полягає в тому, що правильна гіпотеза $H_0$, 
    згідно з вибраним критерієм, відхиляється, називається \emph{помилкою першого роду} --- це так званий <<хибно позитивний висновок>>. 
    \emph{Помилка другого роду} відбувається тоді, коли справджується 
    деяка альтернативна гіпотеза, але приймається основна гіпотеза $H_0$ --- це так званий <<хибно негативний висновок>>.
\end{definition}
% https://cdn-images-1.medium.com/max/1600/0*icEkiqBkLk38GPwM.jpg
Ці помилки істотно різні за своєю суттю. Проілюструємо відмінність 
між названими помилками на прикладі перевірки медичного препарату на дієвість.
В ролі ГС тут $\xi$ --- кількість випадків, коли препарат не подіяв. Висунемо дві гіпотези:
$H_0$ --- <<препарат недієвий>> та $H_1$ --- <<препарат дієвий>>. Помилка першого роду тут полягає в тому, що препарат дійсно не є дієвим,
але ця гіпотеза відхиляється (і це є тим самим хибно позитивним висновком, що препарат дієвий), 
а помилка другого роду --- в тому, що насправді дієвий препарат таким не вважають (це хибно негативний висновок про те, що препарат недієвий).
Наслідки помилки першого роду є випуск у виробництво недієвого (а може, й шкідливого) препарату, а помилки другого роду --- <<непомічання>> 
його дієвості та проведення наступних спроб вдосконалити препарат. Отже, помилка першого роду –-- це помилка, якої важливіше 
уникнути.

Слід зазначити, що статистичний критерій, за яким перевіряється певна 
гіпотеза, не відповідає на питання, правильна гіпотеза чи ні. За критерієм ми 
лише вирішуємо, чи суперечать вибіркові дані висунутій гіпотезі, чи ні. Висновок <<дані 
суперечать гіпотезі>> вважаються більш вагомим, ніж <<дані не суперечать гіпотезі>>.

\subsection{Методика перевірки статистичних гіпотез}
Нехай $\vec{\xi} = \left(\xi_1, \xi_2 ,..., \xi_n\right)$ --- випадкова вибірка, $\xi$ --- ГС, а $G\subseteq \mathbb{R}^n$ --- вибірковий простір.
В залежності від задачі формулюємо основну гіпотезу $H_0$ та альтернативну $H_1$. Сформулюємо загальний принцип побудови 
критеріїв перевірки гіпотез за певною реалізацією вибірки $\vec{x} = \left(x_1, x_2, ..., x_n\right)$.
Критерій задають за допомогою \emph{критичної множини} $W$, що є підмножиною вибіркового простору.
Множину $\overline{W} = G\setminus W$ назвемо \emph{областю прийняття гіпотези}. Рішення приймають так:
\begin{enumerate}
    \item Якщо вибірка $\vec{x} = \left(x_1, x_2, ..., x_n\right)$ належить критичній множині $W$, то вважають, що \emph{дані суперечать основній гіпотезі}, тобто $H_0$ відхиляють.
    \item Якщо вибірка $\vec{x} = \left(x_1, x_2, ..., x_n\right)$ належить області прийняття гіпотези $G\setminus W$, то відхиляють альтернативну гіпотезу й приймають основну $H_0$. В цьому
    випадку вважають, що \emph{дані не суперечать основній гіпотезі}.
\end{enumerate}

Знайдемо ймовірності помилок першого та другого роду. Нехай гіпотези $H_i, i = 0, 1$ полягають в тому, що щільність розподілу $\xi$ дорівнює $f_i(x)$,
якщо $\xi$ --- неперервна, або $\P\left\{ \xi_ = x_k\right\} = p_i(x_k)$, якщо $\xi$ --- дискретна.

\textbf{Ймовірність помилки першого роду} відносно нульової гіпотези $H_0$ дорівнює ймовірності того, що $\vec{\xi}$ потрапить в критичну множину $W$, тобто,
$\P\left\{ \vec{\xi} \in W / H_0\right\} = \alpha$. Величина $\alpha$ називається \emph{рівнем значущості критерію}. Якщо 
$\mathcal{L}_{H_0}(\vec{x})$ --- функція правдоподібності, побудована згідно висунутій гіпотезі $H_0$, то ймовірність помилки першого роду дорівнює
$\int_W \mathcal{L}_{H_0}(\vec{x}) d\vec{x}$. Рівень значущості, як правило, задається.

\textbf{Ймовірність помилки другого роду} відносно нульової гіпотези $H_0$ дорівнює ймовірності того, що $\vec{\xi}$ потрапить в $\overline{W}$,
якщо насправді вірна гіпотеза $H_1$, тобто, $\P\left\{ \vec{\xi} \in \overline{W} / H_1\right\} = \beta$. Якщо 
$\mathcal{L}_{H_1}(\vec{x})$ --- функція правдоподібності, побудована згідно альтернативній гіпотезі $H_1$, то ймовірність помилки другого роду дорівнює
$\int_{\overline{W}} \mathcal{L}_{H_1}(\vec{x}) d\vec{x}$.

Одночасно взяти якомога малими $\alpha$ та $\beta$ неможливо, тому критерій будують таким, щоб величина $1-\beta$, яка називається
\emph{потужністю критерію}, була найбільшою при заданому рівні значущості $\alpha$. Потужність критерію --- це ймовірність відхилити 
основну гіпотезу, якщо вона є хибною.

\section{Критерій Пірсона (\texorpdfstring{$\chi^2$}{x2}) та критерій Колмогорова}
Перейдемо до найважливішої задачі математичної статистики: перевірки гіпотези про закон розподілу генеральної сукупності за певною 
реалізацію вибірки. Припущення про вид закону генеральної сукупності варто висувати тільки після первинної обробки статистичних даних. 
Параметри розподілу, як правило, невідомі, тому їх замінюємо на найкращі точкові оцінки. Очевидно, що між теоретичними та емпіричними 
розподілами існують розходження. Важливо зрозуміти, чи пояснюються ці розходження тільки випадковими обставинами (наприклад, обмеженість 
кількості спостережень), чи суттєвими (наприклад, гіпотетичний закон підібрано невдало).

\subsection{Критерій Пірсона та алгоритм його використання}
Висунемо гіпотезу $H_0: \xi \text{ \emph{має функцію розподілу} } F(x, \theta_1, \theta_2, ..., \theta_m)$.
Згідно з цією гіпотезою $\xi$ може приймати значення з множини $X$, яку розіб'ємо на $r$ підмножин $X_i$, що попарно не перетинаються:
$X = \bigcup\limits_{i=1}^r X_i$ (рекомендації щодо вибору цих множин та їх кількості розглянемо пізніше). 
В припущенні, що гіпотеза $H_0$ справджується, можемо обчислити ймовірності
$p_i = \P\left\{\xi \in X_i / H_0 \right\}$, причому $\sum\limits_{i=1}^r p_i = 1$. Якщо $H_0$ справджується, то частості
$\frac{n_i}{n}$, де $n_i = \sum\limits_{k=1}^n 1{\left\{\xi_k \in X_i \right\}}$ (кількість значень, що потрапили в $X_i$),
мають прямувати за ймовірністю до $p_i$. Критерій Пірсона з'ясовує, чи можна вважати розходження між теоретичними ймовірностями $p_i$
та практичними значеннями $\frac{n_i}{n}$ випадковими, а не систематичними. 

Розглядається статистика 
\begin{gather}
    \eta = \sum_{i=1}^r \frac{n}{p_i}\left(\frac{n_i}{n} - p_i \right)^2 = \sum_{i=1}^r \frac{\left(n_i - np_i\right)^2}{np_i}
\end{gather}

За \emph{теоремою Пірсона}, доведення якої буде наведено далі, якщо складна гіпотеза $H_0$ про закон розподілу генеральної сукупності 
справджується, то статистика $\eta$ прямує за розподілом до розподілу $\chi^2_{r-s-1}$, де $s$ --- кількість невідомих параметрів гіпотетичного закону розподілу,
які оцінюємо, а $r$ --- кількість множин, за якими рахувалися теоретичні ймовірності $p_i$.

\textbf{Алгоритм використання критерію Пірсона.}
\begin{enumerate}
    \item Після висування гіпотези про закон розподілу $\xi$ розбиваємо множину $X$ можливих значень $\xi$ на $r$ підмножин, що попарно не перетинаються.
    \item Згідно висунутій гіпотезі обчислюємо $p_i = \P\left\{\xi \in X_i / H_0 \right\}$, перевіряємо $\sum\limits_{i=1}^r p_i = 1$.
    Якщо $r\geq 20$, то потрібно, щоб виконувалося $n p_i \geq 5, i = 1, ..., r$, а якщо $r<20$ --- $n p_i \geq 10, i = 1, ... r$. Це пов'язано з тим, що
    заміна розподілу $\eta$ на $\chi^2_{r-s-1}$ є наближеною. Якщо ці умови не виконуються, то сусідні підмножини об'єднують, зменшуючи $r$ та збільшуючи відповідні $p_i$.
    \item Обчислюємо $\eta_{\text{зн.}}$ (з, можливо, новим значенням $r$) та порівнюємо з \emph{критичним значенням} $t_{\text{кр.}}$, яке для заданого рівня значущості $\alpha$
    шукається як значення $t_{\alpha, r-s-1}$ з таблиці на ст. \pageref{tabel:chi_2}. Формально, $t_{\text{кр.}}$ --- це квантиль рівня $1-\alpha$ для розподілу $\chi^2_{r-s-1}$.
    \item Критична область в критерії Пірсона є \emph{правосторонньою}: якщо $\eta_{\text{зн.}} < t_{\text{кр.}}$, то дані не суперечать висунутій гіпотезі $H_0$, а якщо
    $\eta_{\text{зн.}} \geq t_{\text{кр.}}$, то суперечать на рівні значущості $\alpha$. Це означає, що відхилення частостей від теоретичних ймовірностей $p_i$ не можна вважати випадковим.
\end{enumerate}

Наведемо декілька прикладів застосування критерію Пірсона.
\begin{example}
    \begin{enumerate}
        \item Спостереження ГС наведені в інтервальному варіаційному ряді. Перевірити гіпотезу про нормальний розподіл ГС
        на рівні значущості $\alpha = 0.05$.
        \begin{center}
            \begin{tabular}{|c|c|c|c|c|}
                \hline
                інтервал & $[-4; 0)$ & $[0; 2)$ & $[2; 4)$ & $[4; 6]$ \\
                \hline
                $n_i$ & $20$ & $40$ & $30$ & $10$ \\
                \hline
            \end{tabular}
        \end{center}
            За реалізацією вибірки знайдемо значення найкращих 
            оцінок параметрів гауссівського розподілу: вибіркового середнього $\overline{x} = 1.4$ та виправленої вибіркової
            дисперсії $D^{**}_{\text{зн.}} = 4.48$. Висунемо гіпотезу $H_0:\xi \sim \mathrm{N}(1.4, 4.48)$. Оскільки $\xi$ за цим припущенням може приймати довільні дійсні значення, то розіб'ємо
            дійсну вісь на інтервали $(-\infty; -4)$, $[-4; 0)$, $[0; 2)$, $[2; 4)$, $[2; 4)$ та $(6; +\infty)$, для яких обчислимо теоретичні ймовірності $p_i$:
            \begin{center}
                \begin{tabular}{|c|c|c|c|c|c|c|}
                    \hline
                    $X_i$ & $(-\infty; -4)$ & $[-4; 0)$ & $[0; 2)$ & $[2; 4)$ & $[4; 6]$ & $(6; +\infty)$\\
                    \hline
                    $p_i$ & $0.005$ & $0.249$ & $0.3575$ & $0.2787$ & $0.0948$ & $0.015$ \\
                    \hline
                    $n p_i$ & $0.5$ & $24.9$ & $35.75$ & $27.87$ & $9.48$ & $1.5$ \\
                    \hline
                \end{tabular}
            \end{center}
            Умова $np_i \geq 10$ не виконується для $(-\infty; -4)$ та $(6; +\infty)$, тому об'єднаємо їх з сусідніми інтервалами:
            \begin{center}
                \begin{tabular}{|c|c|c|c|c|}
                    \hline
                    $X_i$ & $(-\infty; 0)$ & $[0; 2)$ & $[2; 4)$ & $[4; +\infty)$ \\
                    \hline
                    $p_i$ & $0.254$ & $0.3575$ & $0.2787$ & $0.1098$ \\
                    \hline
                    $n p_i$ & $25.4$ & $35.75$ & $27.87$ & $10.98$ \\
                    \hline
                    $n_i$ & $20$ & $40$ & $30$ & $10$ \\
                    \hline
                    $n_i - np_i$ & $-5.4$ & $4.25$ & $2.13$ & $-0.98$ \\
                    \hline
                \end{tabular}
            \end{center}
            Тепер можемо обчислити $\eta_{\text{зн.}} \approx 1.904$. За таблицею розподілу $\chi^2$ знаходимо $t_{\text{кр.}}$ для $4-2-1 = 1$ степені свободи,
            воно рівне $3.84$. Отже, $\eta_{\text{зн.}} < t_{\text{кр.}}$, тому на рівні значущості $0.05$ дані не суперечать гіпотезі про розподіл ГС $\mathrm{N}(1.4, 4.48)$.
            \item Серед 2020 дводітних сімей 527 мають двох хлопчиків, 476 --- двох дівчаток, а у решти 1017 сімей діти різної статі.
            Чи можна на рівні значущості $0.05$ вважати кількість хлопчиків у сім'ї, яка має двох дітей, є 
            біноміально розподіленою випадковою величиною?
            
            Розглянемо випадкову величину $\xi$, яка для кожної дводітної сім'ї, набуває значення $i = 0, 1, 2$, якщо в сім'ї $i$ хлопчиків,
            та висунемо гіпотезу $H_0: \xi \sim \mathrm{Bin}(2, p)$. Параметр $p$ розподілу невідомий, але його точковою оцінкою є
            $p^* = \frac{1}{2} \overline{\xi}$, $p^*_{\text{зн.}} = \frac{0\cdot 476 + 1\cdot 1017 + 2\cdot 527}{2\cdot 2020} \approx 0.513$.
            Вибірковий простір складається з трьох елементів, $X = \left\{0, 1, 2 \right\}$, тому розділимо його на три множини:
            \begin{center}
                \begin{tabular}{|c|c|c|c|}
                    \hline
                    $X_i$ & $\{0\}$ & $\{1\}$ & $\{2\}$ \\
                    \hline
                    $p_i$ & $0.2371$ & $0.4997$ & $0.2632$ \\
                    \hline
                    $n p_i$ & $478.942$ & $1009.394$ & $531.664$ \\
                    \hline
                    $n_i$ & $476$ & $1017$ & $527$ \\
                    \hline
                    $n_i - np_i$ & $-2.942$ & $7.606$ & $-4.664$ \\
                    \hline
                \end{tabular}
            \end{center}
            Обчислимо $\eta_{\text{зн.}} = \frac{(-2.942)^2}{478.942} + \frac{7.606^2}{1009.394} + \frac{(-4.664)^2}{531.664} \approx 0.116$.
            $r = 3$, $s=1$, за таблицею знаходимо $t_{\text{кр.}} = 3.84$.
            Отже, $\eta_{\text{зн.}} < t_{\text{кр.}}$, тому на рівні значущості $0.05$ дані не суперечать гіпотезі про розподіл $\xi \sim \mathrm{Bin}(2, 0.513)$.
    \end{enumerate}
\end{example}

Окремим, більш простішим, випадком застосування критерію $\chi^2$ є його \emph{застосування до схеми Бернуллі}, тобто про перевірці гіпотези про 
ймовірність появи деякої події в послідовності незалежних випробувань. Нехай у послідовності $n$ незалежних випробувань подія $A$ відбулася $m$
разів. Потрібно на рівні значущості $\alpha$ перевірити гіпотезу, що $\P(A) = p$. Дані 
можна розглядати як вибірку з $n$ значень випадкової величини $\xi$, що є індикатором події $A$ у випробуванні.
В цьому випадку статистика $\eta$ запишеться як 
$\eta = \frac{(m-np)^2}{np} + \frac{(n-m-nq)^2}{nq}$, де $q = 1-p$, $m$ --- кількість появ події $A$ у $n$ випробуваннях.
Можна дещо перетворити цю статистику:
$$
\eta = \frac{(m-np)^2}{np} + \frac{(n(1-q) - m)^2}{nq} = \frac{(m - np)^2}{n}\left(\frac{1}{p} + \frac{1}{q}\right) = 
\frac{(m - np)^2}{n p q} = \left( \frac{m - np}{\sqrt{npq}}\right)^2
$$
Згідно ЦГТ $\frac{m - np}{\sqrt{npq}}$ прямує за розподілом до $\mathrm{N}(0, 1)$, тому $\eta$ прямує в такому ж сенсі до $\chi^2_1$.
Далі перевірка основної гіпотези проводиться аналогічно загальному випадку.
\begin{example}
    При $n=4040$ підкиданнях монети <<герб>> випав $m=2048$ разів. Чи узгоджується на рівні значущості $0.1$ з цими даними гіпотеза, що монета симетрична?

    $\eta_{\text{зн.}} = \frac{\left(2048 - 4040/2\right)^2}{4040/2} + \frac{\left(1992 - 4040/2\right)^2}{4040/2} \approx 0.776$.
    За таблицею знайдемо $t_{\text{кр.}} = 2.71$, тому на рівні значущості $0.1$ дані не суперечать гіпотезі про ймовірність випадання <<герба>> $\frac{1}{2}$.
\end{example}

\subsection{Доведення теореми Пірсона}
Нагадаємо <<підготовчу роботу>> для застосування критерію Пірсона.
Висуваємо гіпотезу $H_0: \xi \text{ \emph{має функцію розподілу} } F(x, \theta_1, \theta_2, ..., \theta_m)$.
Згідно з цією гіпотезою $\xi$ може приймати значення з множини $X$, яку розіб'ємо на $r$ підмножин $X_i$, що попарно не перетинаються:
$X = \bigcup\limits_{i=1}^r X_i$, та обчислимо ймовірності $p_i = \P\left\{\xi \in X_i / H_0 \right\}, i = 1, ..., r$.

\begin{gather*}
    \eta = \sum_{i=1}^r \frac{n}{p_i}\left(\frac{n_i}{n} - p_i \right)^2 = \sum_{i=1}^r \frac{\left(n_i - np_i\right)^2}{np_i}
\end{gather*}
\begin{theorem*}[теорема Пірсона]
    В позначеннях, введених вище:
    \begin{enumerate}
        \item Якщо \textbf{проста} гіпотеза $H_0$ щодо закону розподілу ГС справджується,
        то статистика $\eta$ прямує за розподілом до розподілу $\chi^2_{r-1}$ (розподіл $\chi^2$ з $r-1$ ступенями вільності) при $n\to\infty$.
        \item Якщо \textbf{складна} гіпотеза $H_0$ щодо закону розподілу ГС справджується,
        то статистика $\eta$ прямує за розподілом до розподілу $\chi^2_{r-s-1}$ (розподіл $\chi^2$ з $r-s-1$ ступенями вільності) при $n\to\infty$. Тут $s$ --- кількість
        невідомих параметрів розподілу, які оцінюється.
    \end{enumerate}    
\end{theorem*}
Перед доведенням цієї теореми нагадаємо \emph{ЦГТ для випадкових векторів}: нехай $\left\{ \vec{\xi}_n\right\}_{n=1}^{\infty}$ --- послідовність незалежних однаково розподілених випадкових векторів, що мають скінченні
математичне сподівання $\vec{a}$ та кореляційну матрицю $K$. Тоді
\begin{gather}\label{th:clt_vect}
    \sqrt{n}\left( \frac{1}{n} \sum\limits_{k=1}^n \vec{\xi}_k - \vec{a}\right) = 
    \frac{1}{\sqrt{n}} \sum\limits_{k=1}^n \left( \vec{\xi_k} - \vec{a}\right) \overset{\mathrm{F}}{\longrightarrow} \vec{\eta} \sim \mathrm{N}\left(\vec{0}, K\right), \; n\to\infty
\end{gather}
Наслідком ЦГТ є ${\Vert \vec{\eta}_n \Vert}^2 \overset{\mathrm{F}}{\longrightarrow} {\Vert \vec{\eta} \Vert}^2, n\to\infty$, де
$\vec{\eta}_n = \frac{1}{\sqrt{n}} \sum\limits_{k=1}^n \left( \vec{\xi_k} - \vec{a}\right)$, $\vec{\eta} \sim \mathrm{N}\left(\vec{0}, K\right)$.

Також зауважимо, що у випадку $r=2$ теорему вже фактично було доведено, коли розглядалося застосування критерію Пірсона до схеми Бернуллі.
\begin{proof}
    З огляду на технічну складність, проведемо доведення лише для випадку простої гіпотези. 
    План доведення: покажемо, що статистика $\eta$ є квадратом норми деякого вектора, що за ЦГТ збігається за розподілом до $r-1$-вимірного
    стандартного гауссівського вектора, квадрат норми якого, в свою чергу, має розподіл $\chi^2_{r-1}$, звідки отримаємо твердження теореми за наслідком ЦГТ.
    
    Нехай $H_0$ справджується, гіпотетичну область значень $\xi$ розбиваємо на $r>2$
    підмножин, що попарно не перетинаються: $X = \bigcup\limits_{i=1}^r X_i$, $p_i = \P\left\{\xi \in X_i / H_0 \right\}, i=1,...,r$.
    $\vec{\xi} = \left(\xi_1, \xi_2, ..., \xi_n\right)$ --- випадкова вибірка, введемо індикатори 
    $I_{X_i}(\xi_j) = \begin{cases}
        1, & \xi_j \in X_i \\
        0, & \xi_j \notin X_i
    \end{cases}$.
    Оскільки розподіл усіх $\xi_j$ такий самий, як у $\xi$, то $\E\left(I_{X_i}(\xi_j)\right) = p_i$,
    $\D\left(I_{X_i}(\xi_j)\right) = p_i (1-p_i) = p_i q_i$.
    
    Введемо $r$-вимірні вектори:
    \begin{gather*}
        \overrightarrow{\mu^{(1)}} = \begin{pmatrix}
            \frac{I_{X_1}(\xi_1) - p_1}{\sqrt{p_1}} \\
            \frac{I_{X_2}(\xi_1) - p_2}{\sqrt{p_2}} \\
            \cdots \\
            \frac{I_{X_r}(\xi_1) - p_r}{\sqrt{p_r}}
        \end{pmatrix}, \;
        \overrightarrow{\mu^{(2)}} = \begin{pmatrix}
            \frac{I_{X_1}(\xi_2) - p_1}{\sqrt{p_1}} \\
            \frac{I_{X_2}(\xi_2) - p_2}{\sqrt{p_2}} \\
            \cdots \\
            \frac{I_{X_r}(\xi_2) - p_r}{\sqrt{p_r}}
        \end{pmatrix}, \;
        ..., \;
        \overrightarrow{\mu^{(n)}} = \begin{pmatrix}
            \frac{I_{X_1}(\xi_n) - p_1}{\sqrt{p_1}} \\
            \frac{I_{X_2}(\xi_n) - p_2}{\sqrt{p_2}} \\
            \cdots \\
            \frac{I_{X_r}(\xi_n) - p_r}{\sqrt{p_r}}
        \end{pmatrix}
    \end{gather*}
    Внаслідок однакового розподілу та незалежності усіх $\xi_j$ всі $\overrightarrow{\mu^{(j)}}$ теж мають однаковий розподіл та незалежні.
    Оскільки $\E\left(I_{X_i}(\xi_j)\right) = p_i$, то $\E \overrightarrow{\mu^{(1)}} = \E \overrightarrow{\mu^{(2)}} = ... = \E \overrightarrow{\mu^{(n)}} = \vec{0}$.
    Знайдемо кореляційну матрицю $\K$ вектора $\overrightarrow{\mu^{(1)}}$:
    \begin{gather*}
        {cov}\left(\frac{I_{X_i}(\xi_1) - p_i}{\sqrt{p_i}}, \frac{I_{X_j}(\xi_1) - p_j}{\sqrt{p_j}}\right) = 
        \frac{1}{\sqrt{p_i p_j}} \E\left(\left(I_{X_i}(\xi_1) - p_i\right)\left(I_{X_j}(\xi_1) - p_j\right)\right) = \\
        = \frac{1}{\sqrt{p_i p_j}} \left(\E\left(I_{X_i}(\xi_1) I_{X_j}(\xi_1)\right) - p_i p_j\right) = 
        \begin{cases}
            \frac{1}{p_i} \D\left(I_{X_i}(\xi_1)\right) = 1 - p_i, & i = j \\
            -\sqrt{p_i p_j}, & i\neq j
        \end{cases}
    \end{gather*}
    Випадок $i\neq j$ тут пояснюється тим, що множини $X_i$ та $X_j$ не перетинаються.
    \begin{gather*}
        \K = \begin{pmatrix}
            1 - p_1 & -\sqrt{p_1 p_2} & \cdots & -\sqrt{p_1 p_r} \\
            -\sqrt{p_2 p_1} & 1 - p_2 & \cdots & -\sqrt{p_2 p_r} \\
            \vdots & \vdots & \ddots & \vdots \\
            -\sqrt{p_r p_1} & -\sqrt{p_r p_2} & \cdots & 1 - p_r
        \end{pmatrix} = \mathbb{I}_r - 
        \begin{pmatrix}
            -\sqrt{p_1} \\
            -\sqrt{p_2} \\
            \vdots \\
            -\sqrt{p_r}
        \end{pmatrix} 
        \begin{pmatrix}
            -\sqrt{p_1} & -\sqrt{p_2} & \cdots & -\sqrt{p_r}
        \end{pmatrix}
    \end{gather*}
    Тут $\mathbb{I}_r$ --- одична матриця розмірності $r\times r$.
    Виявляється, що координати $\overrightarrow{\mu^{(1)}}$ лінійно залежні:
    \begin{gather*}
        \sum_{i=1}^r \sqrt{p_i} \overrightarrow{\mu^{(1)}_i} = 
        \sum_{i=1}^r \left(I_{X_i}(\xi_1) - p_i\right) = 
        \sum_{i=1}^r I_{X_i}(\xi_1) - 1 = 0 \text{ з ймовірністю } 1
    \end{gather*}
    Це означає, що матриця $\K$ вироджена і має ранг $n-1$: одночасно більше ніж одну координату виразити через інші неможливо,
    оскільки $X_1, X_2, ..., X_r$ попарно не перетинаються. 
    
    Отже, можна знайти таку ортогональну матрицю $U$, що
    вектор $\widehat{\mu^{(1)}} = U \overrightarrow{\mu^{(1)}}$ матиме нульову останню координату. З міркувань про лінійну залежність координат вихідного вектора
    бачимо, що такою матрицею може бути та, останній рядок якої дорівнює 
    $\begin{pmatrix}
        \sqrt{p_1} & \sqrt{p_2} & \cdots & \sqrt{p_r}
    \end{pmatrix}$ (його норма, очевидно, рівна $1$), а інші складаються з ортонормованого базису ортогонального доповнення до лінійної оболонки цього рядка.
    Покажемо, що якими б не були перші $r-1$ рядків матриці $U$, кореляційна матриця $\widehat{\K} = U \K U^{T}$ вектора $\widehat{\mu^{(1)}}$ матиме
    вигляд $\begin{pmatrix}
        \mathbb{I}_{r-1} & 0 \\
        0 & 0 
    \end{pmatrix}$.

    $U = \left(u_{ij}\right)_{i,j = 1, ... n}$, її ортогональність означає, що для всіх $m \neq r$, $l \neq m$
    \begin{gather*}
        \sum\limits_{j=1}^r u_{mj} u_{rj} = \sum\limits_{j=1}^r u_{mj} \sqrt{p_j} = 0, \;
        \sum\limits_{j=1}^r u_{mj}^2 = 1, \;  \sum\limits_{j=1}^r u_{mj} u_{lj} = 0
    \end{gather*}
    Позначимо $i,j$-тий елементи матриць $\K$ та $\widehat{\K}$ через $\sigma_{ij}$ та $\widehat{\sigma}_{ij}$, тоді
    \begin{gather*}
        \widehat{\sigma}_{ml} = \sum\limits_{i=1}^r \left(\sum\limits_{j=1}^r u_{mj}\sigma_{ji}\right)u_{li} = 
        \sum\limits_{i=1}^r \left(\left(\sum\limits_{i\neq j}
            -u_{mj} \sqrt{p_i p_j}\right) + u_{mi}(1-p_i)
        \right) u_{li} = \\
        = \sum\limits_{i=1}^r \left(
            \sqrt{p_i}\left(
                -\sum\limits_{j\neq i} u_{mj}\sqrt{p_j} - u_{mi}\sqrt{p_i}
            \right) + u_{mi}
        \right) u_{li} = \\
        = \sum\limits_{i=1}^r u_{li} \cdot
        \begin{cases}
            u_{mi}, & m\neq r \\
            0, & m = r
        \end{cases} = 
        \begin{cases}
            1, & m \neq r \text{ та } m = l \\
            0, & m = r \text{ або } m \neq l
        \end{cases} \Longrightarrow \widehat{\K} = \begin{pmatrix}
            \mathbb{I}_{r-1} & 0 \\
            0 & 0 
        \end{pmatrix}
    \end{gather*}
    Розглянемо $r$-вимірний випадковий вектор 
    $\overrightarrow{\eta^{(n)}} = \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n \overrightarrow{\mu^{(i)}}$. Його перша координата дорівнює
    \begin{gather*}
        \overrightarrow{\eta^{(n)}_1} = \frac{1}{\sqrt{n}}\left(
            \frac{I_{X_1}(\xi_1) - p_1}{\sqrt{p_1}} +
            \frac{I_{X_1}(\xi_2) - p_1}{\sqrt{p_1}} + ...
            + \frac{I_{X_1}(\xi_n) - p_1}{\sqrt{p_1}}
        \right) = \frac{n_1 - n p_1}{\sqrt{n p_1}}
    \end{gather*}
    Аналогічно
    \begin{gather*}
        \overrightarrow{\eta^{(n)}_2} = \frac{n_2 - n p_2}{\sqrt{n p_2}},
        \overrightarrow{\eta^{(n)}_3} = \frac{n_3 - n p_3}{\sqrt{n p_3}}, ..., 
        \overrightarrow{\eta^{(n)}_r} = \frac{n_r - n p_r}{\sqrt{n p_r}}
    \end{gather*}
    Застосуємо до цього вектора матрицю $U$:
    \begin{gather*}
        \widehat{\eta^{(n)}} = U \overrightarrow{\eta^{(n)}} = \frac{1}{\sqrt{n}}
        \sum\limits_{i=1}^n U \overrightarrow{\mu^{(i)}} = 
        \frac{1}{\sqrt{n}} \sum\limits_{i=1}^n \widehat{\mu^{(i)}}
    \end{gather*}
    Оскільки усі $\overrightarrow{\mu^{(i)}}$ незалежні та однаково розподілені, то усі $\widehat{\mu^{(i)}}$ --- теж, причому
    математичні сподівання $\E \widehat{\mu^{(i)}} = U \E \overrightarrow{\mu^{(i)}} = \vec{0}$,
    а кореляційні матриці дорівнюють $\widehat{K} = \begin{pmatrix}
        \mathbb{I}_{r-1} & 0 \\
        0 & 0 
    \end{pmatrix}$.

    Отже, за ЦГТ для випадкових векторів (\ref{th:clt_vect})
    $\widehat{\eta^{(n)}} \overset{\mathrm{F}}{\longrightarrow} \vec{\eta} \sim \mathrm{N}\left(\vec{0}, \widehat{\K}\right), \; n\to\infty$, 
    а за наслідком з неї
    ${\Big\Vert \widehat{\eta^{(n)}}\Big\Vert}^2 \overset{\mathrm{F}}{\longrightarrow} {\Vert \vec{\eta} \Vert}^2 \sim \chi^2_{r-1}$.
    Залишилося зауважити, що ортогональне перетворення зберігає норму, тому
    $
        {\Big\Vert \widehat{\eta^{(n)}}\Big\Vert}^2 = {\Big\Vert \overrightarrow{\eta^{(n)}}\Big\Vert}^2 = 
        \sum\limits_{i=1}^r \frac{\left(n_i - n p_i\right)^2}{n p_i}
    $ збігається за розподілом до $\chi^2_{r-1}$, що і треба було довести.
\end{proof}

\begin{remark}
    Можна навести нестроге пояснення того, що у випадку складної гіпотези граничним розподілом статистики $\eta$
    є $\chi^2_{r-s-1}$, а не $\chi^2_{r-1}$. Кількість ступенів свободи у випадку простої гіпотези дорівнює $r-1$
    через те, що одну з координат кожного вектора $\overrightarrow{\mu^{(j)}}$ можна було лінійно виразити через інші, але дві чи більше
    координат одночасно так виразити вже неможливо --- це, власне, пояснює термін <<степені свободи>> в цьому випадку: значення однієї фіксованої координати
    цих векторів <<автоматично>> визначається, коли відомі значення інших $r-1$. У випадку складної гіпотези, коли $s$ невідомих параметрів розподілу оцінюються,
    додається ще $s$ подібних обмежень, що зменшує кількість ступенів свободи у граничного розподілу. Дещо подібне відбувалося при побудові довірчих інтервалів
    для дисперсії гауссівської ГС (ст. \pageref{normal_variance_conf_interv}): там використання вибіркового середнього замість точного значення математичного сподівання
    теж зменшувало на $1$ кількість ступенів свободи у розподілі вибіркової дисперсії.
\end{remark}

\subsection{Критерій незалежності \texorpdfstring{$\chi^2$}{x2}}
Наведемо без доведення ще один напрям застосування розподілу $\chi^2$ для перевірки статистичних гіпотез.
Нехай є $n$ спостережень двох випадкових величин $\xi$ та $\eta$, що позначимо відповідно
$\left(x_1, x_2, ..., x_n\right)$ та $\left(y_1, y_2 ,... ,y_n\right)$. Перевірятимемо гіпотезу
$H_0 : \xi \text{\emph{ та }} \eta \text{\emph{ незалежні}}$. 
Як побачимо далі, розподіли $\xi$ та $\eta$ знати непотрібно, тому будемо вважати, що вони приймають скінченну кількість значень
$\left\{X_1, X_2, ... , X_l\right\}$ та $\left\{Y_1, Y_2, ... , Y_k\right\}$ --- інакше спостереження можна згрупувати в деякі інтервали.

Якщо гіпотеза $H_0$ справджується, то $\P\left\{\xi = X_i, \eta = Y_j \right\} = \P\left\{\xi = X_i\right\} \cdot \P\left\{\eta = Y_j\right\}$.
Позначимо $\nu_{ij}$ кількість таких спостережень обох величин, що одночасно спостереження $\xi$ дорівнює $X_i$ і
спостереження $\eta$ дорівнює $Y_j$, $\nu_i^{\xi}$ --- кількість спостережень $\xi$, рівних $X_i$, $\nu_j^{\eta}$ -- кількість спостережень $\eta$, рівних $Y_j$.
Оскільки за ЗВЧ $\frac{\nu_i^\xi}{n} \overset{\mathrm{P}}{\longrightarrow} \P\left\{\xi = X_i\right\}$, 
$\frac{\nu_j^\eta}{n} \overset{\mathrm{P}}{\longrightarrow} \P\left\{\eta = Y_j\right\}$ та 
$\frac{\nu_{ij}}{n} \overset{\mathrm{P}}{\longrightarrow} \P\left\{\xi = X_i, \eta = Y_j \right\}$ при $n\to\infty$,
то вже можна здогадатися, що критерій незалежності базуватиметься на різниці між
$\frac{\nu_{ij}}{n}$ та $\frac{\nu_i^\xi}{n} \cdot \frac{\nu_j^\eta}{n}$. Має місце теорема:
\begin{theorem*}
    Якщо наведена гіпотеза $H_0$ справджується, то статистика
    \begin{gather}
        \zeta = \sum\limits_{i=1}^{l} \sum\limits_{j=1}^{k} \frac{\left(\nu_{ij} - \nu_i^\xi\cdot \nu_j^\eta / n\right)^2}{\nu_i^\xi\cdot \nu_j^\eta / n}
    \end{gather}
    прямує за розподілом до розподілу $\chi^2_{(l-1)(k-1)}$ при $n\to\infty$.
\end{theorem*}

Усі спостереження вносять до так званої \emph{таблиці спряженості}, застосування якої розглянемо на прикладі. Прийняття чи відхилення основної гіпотези
відбувається на основі порівняння значення статистики критерію з критичним значенням, як і випадку звичайного критерію $\chi^2$.

\begin{example}
    Проведено 300 спостережень одночасно над випадковими величинами $\xi$ та $\eta$, які набувають значень $1, 2$ та $1, 2, 3$ відповідно.
    \begin{center}
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            \diagbox{$\xi$}{$\eta$} & 1 & 2 & 3 & $\nu_i^{\xi}$ \\
            \hline
            1 & 32 & 68 & 50 & 150 \\
            \hline
            2 & 40 & 70 & 40 & 150\\
            \hline
            $\nu_j^{\eta}$ & 72 & 138 & 90 & 300\\
            \hline
        \end{tabular}
    \end{center}
    Перевірити за критерієм $\chi^2$ гіпотезу про незалежність $\xi$ та $\eta$ на рівні значущості $0.01$.

    Спочатку знайдемо величини $m_{ij} = \nu_i^\xi\cdot \nu_j^\eta / n$:
    \begin{gather*}
        m_{1 1} = m_{2 1} = \frac{150 \cdot 72}{300} = 36, \; m_{1 2} = m_{2 2} = \frac{150 \cdot 138}{300} = 69, \;
        m_{1 3} = m_{2 3} = \frac{150 \cdot 90}{300} = 45
    \end{gather*}
    Обчислимо квадрати відхилень $\left(\nu_{ij} - m_{ij}\right)^2$:
    \begin{gather*}
        m_{1 1} = (32 - 36)^2 = 16, \; m_{1 2} = (68 - 69)^2 = 1, \; m_{1 3} = (50 - 45)^2 = 25 \\
        m_{2 1} = (40 - 36)^2 = 16, \; m_{2 2} = (70 - 69)^2 = 1, \; m_{2 3} = (40 - 45)^2 = 25
    \end{gather*}
    Значення статистика критерію:
    \begin{gather*}
        \zeta_{\text{зн.}} = 2\cdot \left(\frac{16}{36} + \frac{1}{69} + \frac{25}{45}\right) \approx 2.029
    \end{gather*}
    Кількість ступенів вільності дорівнює $2$, тому за таблицею знаходимо $t_{\text{кр.}} = 9.21$. Отже,
    $\zeta_{\text{зн.}} < t_{\text{кр.}}$ і на рівні значущості $0.01$ дані не суперечать гіпотезі про незалежність $\xi$ та $\eta$.
\end{example}

\subsection{Критерій згоди Колмогорова}
Розглянемо критерій згоди, що використовується для перевірки \emph{простих} гіпотез щодо \emph{неперервного} розподілу ГС: наприклад,
$H_0 : \xi \sim \mathrm{N}(0, 1)$ чи $H_0 : \xi \sim \mathrm{U}\left<-1, 1\right>$.
Вводиться статистика $D_n = \underset{x \in \mathbb{R}}{\sup} \left| F_n^*(x) - F_\xi(x)\right|$, де $F_n^*(x)$ --- емпірична функція розподілу.
На тому, що $F_n^*(x)$ з ймовірністю 1 прямує до $F_\xi(x)$ при $n\to\infty$, базується наступна теорема:

\begin{theorem*}[теорема Колмогорова]
    Нехай $\vec{\xi} = \left(\xi_1, \xi_2, ..., \xi_n\right)$ --- вибірка з неперервної ГС $\xi$ з функцією розподілу $F_{\xi}(x)$, 
    $F_n^*(x)$ --- побудована за вибіркою емпірична функція розподілу. Тоді
    \begin{gather*}
        \underset{n\to\infty}{\lim} \P\left\{ \sqrt{n} \cdot \underset{x \in \mathbb{R}}{\sup} \left| F_n^*(x) - F_\xi(x)\right| < \lambda\right\} = 
        K(\lambda), \;\lambda > 0, 
        \text{ де } K(\lambda) = \sum_{j=-\infty}^{+\infty} (-1)^j e^{-2 j^2 \lambda^2}
    \end{gather*}
\end{theorem*}
Доводити цю теорему не будемо. Зауважимо, що особливістю статистики $D_n$ є те, що її закон розподілу однаковий для всіх неперервних ГС і
залежить лише від обсягу вибірки: якщо зробити у ній заміну $x = F_{\xi}^{-1}(y)$, отримаємо
$D_n = \underset{y \in [0; 1]}{\sup} \left| F_n^*(F_{\xi}^{-1}(y)) - y\right|$. Оскільки випадкові величини $\eta_k = F_{\xi}(\xi_k)$ утворюють
вибірку з розподілу $\mathrm{U}\left<0, 1\right>$, то
\begin{gather*}
    F_n^*(F_{\xi}^{-1}(y)) = \frac{1}{n} \sum_{k=1}^n 1{\left\{ \xi_k < F_{\xi}^{-1}(y)\right\}} = 
    \frac{1}{n} \sum_{k=1}^n 1{\left\{ \eta_k < y\right\}}, \;
    1{\left\{ \eta_k < y\right\}} \sim \mathrm{Bin}(1, y) \text{ при } y \in [0; 1]
\end{gather*}
$\eta_k$ незалежні, тому $\sum\limits_{k=1}^n 1{\left\{ \eta_k < y\right\}} \sim \mathrm{Bin}(n, y)$. Подальші перетворення цієї випадкової величини
мають невипадковий характер, тому, дійсно, розподіл $D_n$ не залежить від розподілу $\xi$.
Таким чином, якщо $H_0$ справджується, то значення $D_n$ буде досить малим. Значення $K(\lambda)$ знаходять з відповідної таблиці.

\textbf{Алгоритм використання критерію Колмогорова.}
\begin{enumerate}
    \item Висуваємо просту гіпотезу $H_0$ про неперервний закон розподілу ГС $\xi$.
    \item За реалізацією вибірки будуємо емпіричну функцію розподілу $F_n^*(x)$.
    \item Знаходимо значення статистики $D_n$ та $\lambda = \sqrt{n} \cdot D_n$.
    \item За заданим рівнем значущості $\alpha$ знаходимо таке $\lambda_\alpha$, що
    $\P\left\{\sqrt{n}\cdot D_n \leq \lambda_\alpha / H_0\right\} = 1 - K(\lambda_\alpha) = \alpha$. 
    \item Якщо $\lambda_{\text{зн.}}$ буде більшим за критичне значення $\lambda_\alpha$, то на рівні значущості $\alpha$
    дослідні дані суперечать гіпотезі $H_0$. Інакше вважають, що гіпотезу $H_0$ можна прийняти.
\end{enumerate}

Наведемо таблицю значень $\lambda_\alpha$ для деяких $\alpha$:
\begin{center}
    \input{tables/kolmogorov_table_raw.tex}
\end{center}

\begin{example}
    Нехай реалізацію вибірки подано у вигляді інтервального 
    варіаційного ряду:
    \begin{center}
        \begin{tabular}{*{11}{|c}|}
            \hline
            інтервал & $[0,1)$ & $[1,2)$ & $[2,3)$ & $[3,4)$ & $[4,5)$ & $[5,6)$ & $[6,7)$ & $[7,8)$ & $[8,9)$ & $[9,10]$ \\
            \hline
            $n_i$ & 35 & 16 & 15 & 17 & 17 & 19 & 11 & 16 & 30 & 24 \\
            \hline
        \end{tabular}
    \end{center}
    На рівні значущості $\alpha = 0.05$ перевірити гіпотезу $H_0: \xi \sim \mathrm{U}\left<0, 10\right>$.

    За інтервальним варіаційним рядом можемо обчислити значення емпіричної та теоретичної функції розподілу в правих кінцях інтервалів,
    а також різниці між ними:
    \begin{center}
        \begin{tabular}{*{11}{|c}|}
            \hline
            $x$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
            \hline
            $F_n^*(x)$ & $0.175$ & $0.255$ & $0.33$ & $0.415$ & $0.5$ & $0.595$ & $0.65$ & $0.73$ & $0.88$ & $1$ \\
            \hline
            $F_\xi(x)$ & $0.1$ & $0.2$ & $0.3$ & $0.4$ & $0.5$ & $0.6$ & $0.7$ & $0.8$ & $0.9$ & $1$ \\
            \hline
            $F_n^*(x) - F_\xi(x)$ & $0.07$5 & $0.055$ & $0.03$ & $0.015$ & $0$ & $-0.004$ & $-0.05$ & $-0.07$ & $-0.02$ & $0$ \\
            \hline
        \end{tabular}
    \end{center}
    $\left(D_n\right)_{\text{зн.}} = 0.075$, тому $\lambda_{\text{зн.}} = \sqrt{200} \cdot 0.075 \approx 1.06$.
    За таблицею $\lambda_{0.05} = 1.358$. Отже, на рівні значущості $0.05$ дані не суперечать гіпотезі про рівномірний
    розподіл $\xi$ на інтервалі $\left<0, 10\right>$.
\end{example}