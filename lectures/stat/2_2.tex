% !TEX root = ../../main.tex
\section{Перевірка параметричних гіпотез}
\subsection{Прості гіпотези. Критерій Неймана-Пірсона}

Часто розподіл генеральної сукупності відомий і за вибіркою треба перевірити 
припущення щодо значень параметрів цього розподілу. Такі гіпотези називають 
\emph{параметричними}. Методика перевірки простої параметричної гіпотези $H_0$
проти простої альтернативної гіпотези $H_1$ ґрунтується на тому, що критичну область $W$
слід вибирати таким чином, щоб ймовірність попадання в неї статистики критерію була мінімальною і 
дорівнювала рівню значущості $\alpha$, якщо дослідні дані не суперечать нульовій гіпотезі $H_0$ та 
максимальною у протилежному випадку. 
Тобто критична область повинна бути такою, щоб при заданому рівні значущості $\alpha$ потужність критерію
$1-\beta$ була максимальною. Назвемо \emph{найкращою критичною областю (НКО)} таку множину, що забезпечує максимальну 
потужність критерію. Побудова такої області ґрунтується на \emph{лемі Неймана-Пірсона}.
\begin{theorem*}[лема Неймана-Пірсона]
    Серед усіх критеріїв заданого рівня значущості $\alpha$, які 
    перевіряють просту параметричну гіпотезу $H_0 : \theta = \theta_0$ проти простої альтернативної гіпотези 
    $H_1: \theta = \theta_1$ \textbf{критерій відношення правдоподібності} є найбільш потужним, тобто, найкраща критична область
    має вигляд
    \begin{gather}
        W_{C_\alpha} = \left\{
            \vec{x} \in \mathbb{R}^n : \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} \geq C_{\alpha}
         \right\}
    \end{gather}
    де у чисельнику стоїть функція правдоподібності в припущенні, що справджується 
    гіпотеза $H_1$, а у знаменнику --- якщо справджується $H_1$. Стала $C_{\alpha}$ вибирається з умови
    \begin{gather}
        \P \left\{ \frac{\mathcal{L}_{H_1}(\vec{\xi})}{\mathcal{L}_{H_0}(\vec{\xi})} \geq C_{\alpha} \middle/ H_0\right\} = 
        \P \left\{ \ln\frac{\mathcal{L}_{H_1}(\vec{\xi})}{\mathcal{L}_{H_0}(\vec{\xi})} \geq \ln C_{\alpha} \middle/ H_0\right\} = \alpha
    \end{gather}
\end{theorem*}
\begin{remark}
    Функція $\frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})}$ називається
    \emph{відношенням правдоподібності}, а $\ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})}$ ---
    \emph{логарифмічним відношенням правдоподібності}.
\end{remark}
\begin{proof}
    Як було зазначено на початку розділу, ймовірність помилки першого роду при заданій критичній множині $W_{C_\alpha}$ можна знайти як
    $\alpha = \P\left\{ \vec{\xi} \in W_{C_\alpha} / H_0\right\} = \int_{W_{C_\alpha}} \mathcal{L}_{H_0}(\vec{x}) d\vec{x}$.
    Нехай $W$ --- деяка інша критична множина рівня значущості $\alpha$, 
    $\P\left\{ \vec{\xi} \in W / H_0\right\} \leq \alpha$. Позначимо $I_W(\vec{x})$ та $I_{W_{C_\alpha}}(\vec{x})$ індикатори цих множин і
    розглянемо функцію 
    $$f(\vec{x}) = \left(I_{W_{C_\alpha}}(\vec{x}) - I_W(\vec{x})\right)\cdot \left(\mathcal{L}_{H_1}(\vec{x}) - C_{\alpha} \cdot \mathcal{L}_{H_0}(\vec{x})\right)$$
    Якщо $\vec{x} \in W_{C_\alpha}$, то $I_{W_{C_\alpha}}(\vec{x}) - I_W(\vec{x}) = 1$ і 
    $\mathcal{L}_{H_1}(\vec{x}) - C_{\alpha} \cdot \mathcal{L}_{H_0}(\vec{x}) \geq 0$ за побудовою.
    Якщо $\vec{x} \in W$, то $I_{W_{C_\alpha}}(\vec{x}) - I_W(\vec{x}) = -1$ і 
    $\mathcal{L}_{H_1}(\vec{x}) - C_{\alpha} \cdot \mathcal{L}_{H_0}(\vec{x}) \leq 0$ за побудовою.
    В інших випадках ($\vec{x}$ належить обом множинам одночасно або не належить жодній) перший множник дорівнює $0$.
    Отже, $f(\vec{x}) \geq 0$ для всіх $\vec{x} \in \mathbb{R}^n$.
    \begin{gather*}
        0 \leq \int_{\mathbb{R}^n} f(\vec{x}) d\vec{x} = 
        \int_{\mathbb{R}^n} \left(I_{W_{C_\alpha}}(\vec{x}) - I_W(\vec{x})\right)\cdot \left(\mathcal{L}_{H_1}(\vec{x}) - C_{\alpha} \cdot \mathcal{L}_{H_0}(\vec{x})\right) d\vec{x} =\\
        = \int_{W_{C_\alpha}} \mathcal{L}_{H_1}(\vec{x}) d\vec{x} - \int_{W} \mathcal{L}_{H_1}(\vec{x}) d\vec{x}
        - C_{\alpha} \cdot \int_{W_{C_\alpha}} \mathcal{L}_{H_0}(\vec{x}) d\vec{x} +
        C_{\alpha} \cdot \int_{W} \mathcal{L}_{H_0}(\vec{x}) d\vec{x} = \\
        = \P\left\{ \vec{\xi} \in W_{C_\alpha} / H_1\right\} - \P\left\{ \vec{\xi} \in W / H_1\right\} -
        C_{\alpha}\cdot\left(\P\left\{ \vec{\xi} \in W_{C_\alpha} / H_0\right\} - \P\left\{ \vec{\xi} \in W / H_0\right\}\right)
    \end{gather*}
    $\P\left\{ \vec{\xi} \in W_{C_\alpha} / H_0\right\} - \P\left\{ \vec{\xi} \in W / H_0\right\} \geq 0$, тому
    $\P\left\{ \vec{\xi} \in W_{C_\alpha} / H_1\right\} - \P\left\{ \vec{\xi} \in W / H_1\right\} \geq 0$. 
    Це й означає, що вибір критичної множини $W_{C_\alpha}$ дає більшу потужність критерію, ніж $W$.
\end{proof}
Твердження леми Неймана-Пірсона можна застосовувати для знаходження потужності критерію та обсягу вибірки, що забезпечує заданий рівень значущості та ймовірність помилки другого роду.
Знаючи $C_{\alpha}$, можна знайти ймовірність помилки другого роду $\beta$ як
\begin{gather*}
    \P \left\{ \ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} < \ln C_{\alpha} \middle/ H_1\right\} = \beta
\end{gather*}
Для визначення мінімального обсягу вибірки, при якому досягаються наперед задані ймовірності помилок першого $\alpha$ та другого роду $\beta$,
треба розв'язати систему
\begin{gather*}
    \begin{cases}
        \P \left\{ \ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} \geq \ln C_{\alpha} \middle/ H_0\right\} = \alpha \\
        \P \left\{ \ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} < \ln C_{\alpha} \middle/ H_1 \right\} = \beta
    \end{cases}
\end{gather*}
Відповідні приклади будуть розглянуті нижче (ст. \pageref{np_lemma_examples}).

\subsection{Приклади побудови найкращої критичної області}
\noindent\textbf{Приклад 1.}\label{nko:ex1} Побудувати найкращу критичну область за лемою Неймана-Пірсона
для параметра $a$ нормально розподіленої ГС з відомою дисперсією $\sigma^2$. Розглянемо дві прості гіпотези:
$H_0 : \E\xi = a_0$ та $H_1 : \E\xi = a_1$, де $a_0 < a_1$. Функція правдоподібності має вигляд
$$ \mathcal{L}(x_1, ..., x_n, a) = 
\frac{1}{(2\pi)^{\frac{n}{2}} \sigma^n} \cdot \exp\left\{-\frac{1}{2\sigma^2}\sum\limits_{k=1}^n (x_k - a)^2\right\}$$
Тому звичайне та логарифмічне відношення правдоподібності матимуть вигляд
\begin{gather*}
    \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} = 
    \exp\left\{ -\frac{1}{2\sigma^2}\sum\limits_{k=1}^n (x_k - a_1)^2 + \frac{1}{2\sigma^2}\sum\limits_{k=1}^n (x_k - a_0)^2\right\} = \\
    = \exp\left\{ -\frac{1}{2\sigma^2}\sum\limits_{k=1}^n \left(x_k^2 - 2x_k a_1 + a_1^2 - x_k^2 + 2x_k a_0 - a_0^2\right)\right\} = \\
    = \exp\left\{ -\frac{1}{2\sigma^2} \left( 2(a_1 - a_0) \sum\limits_{k=1}^n x_k + n(a_1^2 - a_0^2)\right)\right\} = 
    \exp\left\{\frac{a_1 - a_0}{\sigma^2}\sum\limits_{k=1}^n x_k \right\}
    \exp\left\{-\frac{n(a_1^2 - a_0^2)}{2\sigma^2}\right\}
\end{gather*}
\begin{gather*}
    \ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} = 
    \frac{a_1 - a_0}{\sigma^2}\sum\limits_{k=1}^n x_k - \frac{n(a_1^2 - a_0^2)}{2\sigma^2} = 
    \frac{n(a_1 - a_0)}{\sigma^2}\cdot\left(\underbrace{\frac{1}{n}\sum\limits_{k=1}^n x_k}_{\overline{x}} - \frac{a_0 + a_1}{2} \right)
\end{gather*}
Розв'яжемо відносно $\overline{x}$ нерівність $\ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} \geq \ln C_{\alpha}$:
\begin{gather*}
    \frac{n(a_1 - a_0)}{\sigma^2}\cdot \left(\overline{x} - \frac{a_0 + a_1}{2}\right) \geq \ln C_{\alpha} \Leftrightarrow
    \overline{x} \geq \frac{\sigma^2 \ln C_{\alpha}}{n(a_1 - a_0)} + \frac{a_0 + a_1}{2} = t_{\text{кр}}
\end{gather*}
Зауважимо, що знак нерівності тут не змінюється, оскільки $a_1 > a_0$. Знайдемо $t$ з рівності
$\P \left\{\overline{\xi} \geq t / H_0 \right\} = \alpha$, користуючись тим, що $\overline{\xi} \sim \mathrm{N}\left(a_0, \frac{\sigma^2}{n}\right)$, 
якщо $H_0$ справджується:
\begin{gather*}
    \P \left\{\overline{\xi} \geq t / H_0 \right\} =  \P \left\{\frac{(\overline{\xi} - a_0)\sqrt{n}}{\sigma} \geq \frac{(t - a_0)\sqrt{n}}{\sigma} \middle/ \overline{\xi} \sim \mathrm{N}\left(a_0, \frac{\sigma^2}{n}\right) \right\} =
     0.5 - \Phi\left(\frac{(t - a_0)\sqrt{n}}{\sigma}\right) = \alpha
\end{gather*}
Знайдене значення $t$ буде межею критичної області, яка в цьому випадку буде \emph{правосторонньою}.
Якщо значення вибіркового середнього, обчисленого за конкретною 
реалізацією, буде більше за $t$, то гіпотезу $H_0 : a = a_0$ треба відхиляти, бо дослідні дані суперечать цій 
гіпотезі на рівні значущості $\alpha$. В іншому випадку гіпотезу $H_0$ приймаємо.

У випадку $a_1 < a_0$ при розв'язанні нерівності відносно $\overline{x}$ зміниться її знак, тому критична область буде \emph{лівосторонньою}.

\begin{remark}
    Якщо дисперсія ГС є невідомою, то треба користуватися статистикою $\eta = \frac{\left(\overline{\xi} - a_0\right)\sqrt{n}}{\sqrt{\D^{**}\xi}}$, 
    що має розподіл $\mathrm{St}_{n-1}$, якщо
    гіпотеза $H_0$ справджується. В цьому випадку
    критична область також буде \emph{правосторонньою} при $a_1 > a_0$, а при $a_1 < a_0$ --- \emph{лівосторонньою}.
\end{remark}

Продемонструємо застосування знайденої НКО на конкретній задачі.

\noindent\textbf{Задача 1.} Стверджується, що кульки, вироблені верстатом, мають 
середній діаметр $10$ мм. Перевірити цю гіпотезу на рівні значущості $\alpha=0.05$, якщо після 
перевірки 16 кульок середнє значення діаметра дорівнювало $10.3$ мм. Вважати, що результати вимірювання $\xi$ нормально розподілені з $\sigma = 1$.
Що зміниться, якщо дисперсія є невідомою, але $\left(\D^{**}\xi\right)_{\text{зн}} = 1$?

З умови задачі висуваємо гіпотези $H_0 : \E \xi = 10$ та $H_1 : \E \xi = a > 10$, бо $\overline{x} > 10$. Обчислимо межу критичної області з рівності
$0.05 = 0.5 - \Phi\left(\frac{(t-10)\sqrt{16}}{1}\right) \Leftrightarrow \Phi\left(4(t-10)\right) = 0.45$. З таблиці значень функції Лапласа
$4(t-10) = 1.65 \Leftrightarrow t = 10.4125$. Отже, $\overline{x} = 10.3 < t$ і на рівні значущості $0.05$ основна гіпотеза приймається.
Якщо дисперсія є невідомою, то межу критичної області знайдемо з $\P\left\{\eta > t_{\text{кр}}\right\} = \alpha$ для $\eta \sim \mathrm{St}_{15}$,
вона дорівнює $1.753$. $\eta_{\text{зн}} = \frac{(10.3 - 10)\cdot 4}{1} = 1.2 < t_{\text{кр}}$. Отже, і в цьому випадку дані не суперечать основній 
гіпотезі на рівні значущості $0.05$. 

\label{np_lemma_examples}
В умовах цієї задачі знайдемо також ймовірність помилки другого роду з рівності
$\beta = \P \left\{ \ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} < \ln C_{\alpha} \middle/ H_1\right\}$.
Повторюючи міркування при знаходженні межі критичної області, отримаємо 
$\beta = \P\left\{ \overline{\xi} < t_\text{кр} \middle/ H_1\right\} = 
0.5 + \Phi\left(\frac{(t_\text{кр} - 10.3)\sqrt{16}}{1}\right) = 
0.5 + \Phi\left(\frac{(10.41 - 10.3)\cdot 4}{1}\right) \approx 0.617$.
Відповідно, потужність критерію дорівнює $1-\beta \approx 0.383$.

Якщо ймовірність помилки другого роду при заданому рівні значущості треба зменшити,
необхідно збільшити обсяг вибірки. Нехай $\beta = 0.1$, а $\alpha = 0.05$, як і було в задачі.
Розв'яжемо систему:
\begin{gather*}
    \begin{cases}
        \P \left\{ \ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} \geq \ln C_{\alpha} \middle/ H_0\right\} = \alpha \\
        \P \left\{ \ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} < \ln C_{\alpha} \middle/ H_1 \right\} = \beta
    \end{cases} \Leftrightarrow
    \begin{cases}
        0.5 - \Phi\left(\frac{(t- 10)\sqrt{n}}{1}\right) = 0.05 \\
        0.5 + \Phi\left(\frac{(t - 10.3)\sqrt{n}}{1}\right) = 0.1
    \end{cases} \Leftrightarrow \\
    \Leftrightarrow
    \begin{cases}
        \Phi\left(t - 10)\sqrt{n}\right) = 0.45 \\
        \Phi\left(t - 10.3)\sqrt{n}\right) = -0.4
    \end{cases}
    \Leftrightarrow
    \begin{cases}
        (t-10) \sqrt{n} \approx 1.64 \\
        (t - 10.3) \sqrt{n} \approx -1.28
    \end{cases}
    \Rightarrow
    \begin{cases}
        \sqrt{n} \approx 9.733 \\
        t \approx 10.17
    \end{cases}
\end{gather*}
Отже, обсяг вибірки $n \geq 95$ забезпечить ймовірність помилки другого роду, рівну $0.1$.


\vspace{3mm}
\noindent\textbf{Приклад 2.} Побудувати найкращу критичну область за лемою Неймана-Пірсона 
для параметра $\sigma^2$ нормального закону розподілу з відомим математичним сподіванням $a$.
Введемо дві прості гіпотези (основну та альтернативну): $H_0 : \D\xi = \sigma_0^2$ та $H_1 : \D\xi = \sigma_1^2$, де $\sigma_0^2 > \sigma_1^2$.
Як і у прикладі \hyperref[nko:ex1]{1}, запишемо логарифмічне відношення правдоподібності:
\begin{gather*}
    \ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} = \ln\left(\frac{\sigma_0}{\sigma_1}\right)^n
    -\frac{1}{2\sigma_1^2}\sum\limits_{k=1}^n (x_k - a)^2 + \frac{1}{2\sigma_0^2}\sum\limits_{k=1}^n (x_k - a)^2 = \\
    = n\cdot \ln \frac{\sigma_0}{\sigma_1} + \frac{n}{2}\cdot\frac{1}{n}\cdot \sum\limits_{k=1}^n (x_k - a)^2
    \cdot \frac{\sigma_1^2 - \sigma_0^2}{\sigma_1^2 \cdot\sigma_0^2} = 
    n\cdot \ln \frac{\sigma_0}{\sigma_1} + \frac{n}{2}\cdot \left(\D^*\xi\right)_{\text{зн}}
    \cdot \frac{\sigma_1^2 - \sigma_0^2}{\sigma_1^2 \cdot\sigma_0^2}
\end{gather*}
Розв'яжемо відносно $\left(\D^*\xi\right)_{\text{зн}}$ нерівність $\ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} \geq \ln C_{\alpha}$:
\begin{gather*}
    \frac{n}{2}\cdot \left(\D^*\xi\right)_{\text{зн}}
    \cdot \frac{\sigma_1^2 - \sigma_0^2}{\sigma_1^2 \cdot\sigma_0^2} \geq \ln C_{\alpha} - n\cdot \ln \frac{\sigma_0}{\sigma_1} \Leftrightarrow
    \left(\D^*\xi\right)_{\text{зн}} \leq
    \frac{\ln C_{\alpha} - n\cdot \ln \frac{\sigma_0}{\sigma_1}}{\frac{n}{2}\cdot\frac{\sigma_1^2 - \sigma_0^2}{\sigma_1^2 \cdot\sigma_0^2} }
\end{gather*}
Зауважимо, що знак нерівності тут змінюється, оскільки $\sigma_1^2 < \sigma_0^2$.
Знайдемо $t$ з рівності
$\P \left\{\frac{n\D^*\xi}{\sigma_0^2} \leq t / H_0 \right\} = \alpha$, користуючись тим, що $\eta = \frac{n\D^*\xi}{\sigma_0^2} \sim \chi^2_n$,
якщо $H_0$ справджується. Знайдене значення $t$ буде межею критичної області, яка в цьому випадку буде \emph{лівосторонньою}.
Якщо значення статистики $\eta$, обчисленої за конкретною 
реалізацією, буде менше за $t$, то гіпотезу $H_0 : \D\xi = \sigma_0^2$ треба відхиляти, бо дослідні дані суперечать цій 
гіпотезі на рівні значущості $\alpha$. В іншому випадку гіпотезу $H_0$ приймаємо.

У випадку $\sigma_1^2 > \sigma_0^2$ при розв'язанні нерівності відносно $\left(\D^*\xi\right)_{\text{зн}}$ її знак не зміниться, тому критична область буде \emph{правосторонньою}.

\begin{remark}
    Якщо математичне сподівання ГС є невідомим, то треба користуватися статистикою $\eta = \frac{(n-1)\D^{**}\xi}{\sigma_0^2}$, 
    що має розподіл $\chi^2_{n-1}$, якщо гіпотеза $H_0$ справджується. В цьому випадку
    критична область також буде \emph{правосторонньою} при $\sigma_1^2 > \sigma_0^2$, а при $\sigma_1^2 < \sigma_0^2$ --- \emph{лівосторонньою}.
\end{remark}

Продемонструємо застосування знайденої НКО на конкретній задачі.

\noindent\textbf{Задача 2.} Точність деякого верстата характеризується дисперсією довжини виготовлених деталей: якщо вона більша за 400 мкм$^2$, то
верстат треба переналаштувати. Після перевірки $n=15$ деталей виявилося, що $\left(\D^{**}\xi\right)_{\text{зн}} = 680$ мкм$^2$. Чи потрібно переналаштувати верстат?

З умови задачі висуваємо основну гіпотезу $H_0 : \sigma^2 = \sigma_0^2 = 400$ і альтернативну $H_1 : \sigma^2 = \sigma_1^2$, де $\sigma_1^2 > 400$.
Оберемо рівень значущості $\alpha = 0.01$. Оскільки математичне сподівання невідоме, скористаємося статистикою $\eta = \frac{(n-1)\D^{**}\xi}{\sigma_0^2}$,
що має розподіл $\chi^2_{n-1}$, якщо гіпотеза $H_0$ справджується. Критична область буде правосторонньою, за таблицею знайдемо її межу $t_{\text{кр}} = 29.14$.
Оскільки $\eta_{\text{зн}} = \frac{14 \cdot 680}{400} = 23.8 < t_{\text{кр}}$, гіпотеза $H_0$ приймається --- верстат не треба переналаштувати.

\vspace{3mm}
\noindent\textbf{Приклад 3.} Побудувати найкращу критичну область для перевірки простої гіпотези про значення параметра $a$
пуассонівської ГС при великому обсягу вибірки.
Маємо основну гіпотезу $H_0 : a = a_0$ та альтернативну $H_1 : a = a_1$, нехай $a_1 > a_0$. Логарифмічна функція правдоподібності має вигляд
\begin{gather*}
    \ln \mathcal{L}(x_1, ..., x_n, a) = -n a + \ln a \cdot \sum\limits_{k=1}^n x_k - \sum\limits_{k=1}^n \ln{x_k!}
\end{gather*}
Запишемо логарифмічне відношення правдоподібності
\begin{gather*}
    \ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} = \ln \mathcal{L}_{H_1}(\vec{x}) - \ln \mathcal{L}_{H_0}(\vec{x}) =
    -n \cdot \left(a_1 - a_0\right) + \ln\left(\frac{a_1}{a_0}\right)\cdot \sum\limits_{k=1}^n x_k = \\
    = -n \cdot \left(a_1 - a_0\right) + n \ln\left(\frac{a_1}{a_0}\right) \cdot \frac{1}{n}\sum\limits_{k=1}^n x_k 
    = -n \cdot \left(a_1 - a_0\right) + n \ln\left(\frac{a_1}{a_0}\right) \cdot \overline{x}
\end{gather*}
Розв'яжемо відносно $\overline{x}$ нерівність $\ln \frac{\mathcal{L}_{H_1}(\vec{x})}{\mathcal{L}_{H_0}(\vec{x})} \geq \ln C_{\alpha}$:
\begin{gather*}
    -n \cdot \left(a_1 - a_0\right) + n \ln\left(\frac{a_1}{a_0}\right) \cdot \overline{x} \geq \ln C_{\alpha} \Leftrightarrow
    \overline{x} \geq \frac{\ln C_{\alpha} + n \cdot \left(a_1 - a_0\right)}{n \ln\left(\frac{a_1}{a_0}\right)}
\end{gather*}
Зауважимо, що знак нерівності тут не змінюється, оскільки $a_1 > a_0$ і тому $\ln\left(\frac{a_1}{a_0}\right) > 0$.
Якщо обсяг вибірки великий, то $\overline{\xi}$ наближено має розподіл $\mathrm{N}\left(a_0, \frac{a_0}{n}\right)$, якщо $H_0$ справджується:
\begin{gather*}
    \P \left\{\overline{\xi} \geq t / H_0 \right\} = \alpha \Rightarrow \alpha \approx 0.5 - \Phi\left(\frac{(t - a_0)\sqrt{n}}{a_0}\right)
\end{gather*}
Знайдене значення $t$ буде межею критичної області, яка в цьому випадку буде \emph{правосторонньою}.
Якщо значення вибіркового середнього, обчисленого за конкретною 
реалізацією, буде більше за $t$, то гіпотезу $H_0 : a = a_0$ треба відхиляти, бо дослідні дані суперечать цій 
гіпотезі на рівні значущості $\alpha$. В іншому випадку гіпотезу $H_0$ приймаємо.

У випадку $a_1 < a_0$ при розв'язанні нерівності відносно $\overline{x}$ зміниться її знак, тому критична область буде \emph{лівосторонньою}.

Зрозуміло, що аналогічні міркування можна застосовувати і для інших розподілів ГС, якщо у логарифмічному відношенні правдоподібності вдається
виділити вибіркове середнє чи іншу статистику, для якої справджується ЦГТ.

\begin{exercise}
    Побудувати за лемою Неймана-Пірсона найкращу критичну область для параметра $p$ ГС $\xi \sim \mathrm{Bin}(N, p)$, вважаючи $N$ відомим, а обсяг вибірки великим.
    Розглянути випадки, коли альтернативне значення $p_1$ більше та менше основного $p_0$.
\end{exercise}